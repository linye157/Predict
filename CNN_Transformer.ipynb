{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成训练集\n",
    "class Train_Loader(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir, \n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        data = pd.read_excel(self.data_dir, skiprows=0)\n",
    "        #将数据类型转换为float\n",
    "        data = data.astype('float32')\n",
    "        #缺失值使用上一行和下一行同一列的平均值填充\n",
    "        data = data.fillna(data.mean())\n",
    "        #对目标列(除最后三列)归一化\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data.iloc[:, :-3] = scaler.fit_transform(data.iloc[:, :-3])\n",
    "        self.data = data.values\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        features = self.data[i, :-3]\n",
    "        #扩充为二维\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        targets = self.data[i, -3:]\n",
    "        #扩充为二维\n",
    "        targets = np.expand_dims(targets, axis=0)\n",
    "        return features, targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "#生成测试集\n",
    "class Test_Loader(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir, \n",
    "            scaler,\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        data = pd.read_excel(self.data_dir, skiprows=0)\n",
    "        #将数据类型转换为float\n",
    "        data = data.astype('float32')\n",
    "        #缺失值使用上一行和下一行同一列的平均值填充\n",
    "        data = data.fillna(data.mean())\n",
    "        #对目标列(除最后三列)归一化\n",
    "        data.iloc[:, :-3] = scaler.transform(data.iloc[:, :-3])\n",
    "        self.data = data.values\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        features = self.data[i, :-3]\n",
    "        #扩充为二维\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        targets = self.data[i, -3:]\n",
    "        #扩充为二维\n",
    "        targets = np.expand_dims(targets, axis=0)\n",
    "        return features, targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir=\"data/train_data.xlsx\"\n",
    "train_data_loader = Train_Loader(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 65)\n",
      "(1, 3)\n",
      "20157\n"
     ]
    }
   ],
   "source": [
    "#查看feature，target数据形状\n",
    "features, targets = train_data_loader[0]\n",
    "print(features.shape)\n",
    "print(targets.shape)\n",
    "#查看train_data_loader数据长度\n",
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 65])\n",
      "torch.Size([128, 1, 3])\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "batchsize=128\n",
    "#创建数据加载器\n",
    "train_data = DataLoader(train_data_loader, batch_size=batchsize, shuffle=True)\n",
    "#查看数据加载器，数据形状\n",
    "for features, targets in train_data:\n",
    "    print(features.shape)\n",
    "    print(targets.shape)\n",
    "    break\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nets.CNN_BIGRU_Attention import CNN_BiGRU_Attention\n",
    "from nets.CNN_Transformer import CNN_Transformer\n",
    "n_future = 65  # Example value, adjust as needed\n",
    "n_class = 3   # Example value, adjust as needed\n",
    "model = CNN_Transformer(n_future, n_class)\n",
    "\n",
    "#定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "#定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# model_dir=\"model/CNN_BiGRU_Attention\"\n",
    "model_dir=\"model/CNN_Transformer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load(\"model/CNN_Transformer247.41838136212579.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "#计算验证集的MSE\n",
    "valid_data_dir=\"data/valid_data.xlsx\"\n",
    "valid_data_loader = Test_Loader(valid_data_dir, train_data_loader.scaler)\n",
    "valid_data = DataLoader(valid_data_loader, batch_size=batchsize, shuffle=False)\n",
    "print(len(valid_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(valid_model, valid_data, criterion):\n",
    "    # valid_model.eval()\n",
    "    valid_loss = 0\n",
    "    for features, targets in valid_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        valid_model = valid_model.cuda()\n",
    "        outputs = valid_model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        valid_loss = valid_loss+loss.item()\n",
    "    # print('valid_loss:', valid_loss / len(valid_data))\n",
    "    return valid_loss / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 40860.6054, Valid Loss: 12137.7983\n",
      "Epoch: 1, Train Loss: 10600.2515, Valid Loss: 1855.5761\n",
      "Epoch: 2, Train Loss: 1009.4238, Valid Loss: 739.0311\n",
      "Epoch: 3, Train Loss: 673.0568, Valid Loss: 559.3743\n",
      "Epoch: 4, Train Loss: 512.6396, Valid Loss: 449.0355\n",
      "Epoch: 5, Train Loss: 406.1927, Valid Loss: 422.8269\n",
      "Epoch: 6, Train Loss: 384.4698, Valid Loss: 406.3339\n",
      "Epoch: 7, Train Loss: 363.9380, Valid Loss: 356.0440\n",
      "Epoch: 8, Train Loss: 342.8370, Valid Loss: 334.6470\n",
      "Epoch: 9, Train Loss: 320.9771, Valid Loss: 315.6983\n",
      "Epoch: 10, Train Loss: 285.1829, Valid Loss: 240.6401\n",
      "Epoch: 11, Train Loss: 214.0111, Valid Loss: 193.9870\n",
      "Epoch: 12, Train Loss: 206.3693, Valid Loss: 360.1801\n",
      "Epoch: 13, Train Loss: 197.7278, Valid Loss: 211.2484\n",
      "Epoch: 14, Train Loss: 176.5073, Valid Loss: 201.5024\n",
      "Epoch: 15, Train Loss: 181.6936, Valid Loss: 168.8872\n",
      "Epoch: 16, Train Loss: 163.0801, Valid Loss: 191.2275\n",
      "Epoch: 17, Train Loss: 157.7598, Valid Loss: 174.6760\n",
      "Epoch: 18, Train Loss: 357.4148, Valid Loss: 248.5102\n",
      "Epoch: 19, Train Loss: 211.6843, Valid Loss: 191.3997\n",
      "Epoch: 20, Train Loss: 187.9581, Valid Loss: 190.0946\n",
      "Epoch: 21, Train Loss: 183.8334, Valid Loss: 177.7166\n",
      "Epoch: 22, Train Loss: 174.8571, Valid Loss: 169.8608\n",
      "Epoch: 23, Train Loss: 167.7822, Valid Loss: 165.4287\n",
      "Epoch: 24, Train Loss: 163.8864, Valid Loss: 160.4384\n",
      "Epoch: 25, Train Loss: 164.8392, Valid Loss: 168.4789\n",
      "Epoch: 26, Train Loss: 471.9952, Valid Loss: 325.7058\n",
      "Epoch: 27, Train Loss: 219.3265, Valid Loss: 197.5062\n",
      "Epoch: 28, Train Loss: 190.0694, Valid Loss: 183.7876\n",
      "Epoch: 29, Train Loss: 180.6991, Valid Loss: 177.6515\n",
      "Epoch: 30, Train Loss: 169.9833, Valid Loss: 171.0938\n",
      "Epoch: 31, Train Loss: 168.7502, Valid Loss: 171.5487\n",
      "Epoch: 32, Train Loss: 177.4508, Valid Loss: 173.4503\n",
      "Epoch: 33, Train Loss: 171.6014, Valid Loss: 161.8639\n",
      "Epoch: 34, Train Loss: 159.4644, Valid Loss: 160.7322\n",
      "Epoch: 35, Train Loss: 154.6634, Valid Loss: 155.6985\n",
      "Epoch: 36, Train Loss: 151.9309, Valid Loss: 176.9676\n",
      "Epoch: 37, Train Loss: 152.5164, Valid Loss: 157.3406\n",
      "Epoch: 38, Train Loss: 154.6769, Valid Loss: 148.8425\n",
      "Epoch: 39, Train Loss: 151.9075, Valid Loss: 158.8656\n",
      "Epoch: 40, Train Loss: 145.7657, Valid Loss: 146.5969\n",
      "Epoch: 41, Train Loss: 144.2506, Valid Loss: 149.3020\n",
      "Epoch: 42, Train Loss: 146.1821, Valid Loss: 147.2585\n",
      "Epoch: 43, Train Loss: 142.2131, Valid Loss: 150.4437\n",
      "Epoch: 44, Train Loss: 142.1723, Valid Loss: 138.1195\n",
      "Epoch: 45, Train Loss: 139.5262, Valid Loss: 134.3611\n",
      "Epoch: 46, Train Loss: 138.5697, Valid Loss: 147.5779\n",
      "Epoch: 47, Train Loss: 138.0759, Valid Loss: 134.2530\n",
      "Epoch: 48, Train Loss: 136.2358, Valid Loss: 134.6358\n",
      "Epoch: 49, Train Loss: 135.0324, Valid Loss: 133.8009\n",
      "Epoch: 50, Train Loss: 139.2436, Valid Loss: 139.2722\n",
      "Epoch: 51, Train Loss: 135.8239, Valid Loss: 142.0491\n",
      "Epoch: 52, Train Loss: 136.2070, Valid Loss: 134.0294\n",
      "Epoch: 53, Train Loss: 143.0950, Valid Loss: 130.9036\n",
      "Epoch: 54, Train Loss: 131.1383, Valid Loss: 129.9501\n",
      "Epoch: 55, Train Loss: 127.0965, Valid Loss: 126.9746\n",
      "Epoch: 56, Train Loss: 129.0977, Valid Loss: 122.6337\n",
      "Epoch: 57, Train Loss: 124.9826, Valid Loss: 124.1707\n",
      "Epoch: 58, Train Loss: 128.5761, Valid Loss: 187.3814\n",
      "Epoch: 59, Train Loss: 137.4843, Valid Loss: 124.2039\n",
      "Epoch: 60, Train Loss: 130.0484, Valid Loss: 124.7205\n",
      "Epoch: 61, Train Loss: 126.6580, Valid Loss: 131.4679\n",
      "Epoch: 62, Train Loss: 122.0558, Valid Loss: 117.4902\n",
      "Epoch: 63, Train Loss: 124.0290, Valid Loss: 151.6602\n",
      "Epoch: 64, Train Loss: 122.2304, Valid Loss: 120.9015\n",
      "Epoch: 65, Train Loss: 118.8643, Valid Loss: 118.4172\n",
      "Epoch: 66, Train Loss: 114.6431, Valid Loss: 112.2612\n",
      "Epoch: 67, Train Loss: 119.5303, Valid Loss: 140.4546\n",
      "Epoch: 68, Train Loss: 117.8134, Valid Loss: 117.1786\n",
      "Epoch: 69, Train Loss: 110.9661, Valid Loss: 114.6893\n",
      "Epoch: 70, Train Loss: 113.2611, Valid Loss: 112.6116\n",
      "Epoch: 71, Train Loss: 116.2196, Valid Loss: 130.6111\n",
      "Epoch: 72, Train Loss: 113.8468, Valid Loss: 109.5296\n",
      "Epoch: 73, Train Loss: 111.0059, Valid Loss: 106.2891\n",
      "Epoch: 74, Train Loss: 112.3411, Valid Loss: 110.7532\n",
      "Epoch: 75, Train Loss: 108.2854, Valid Loss: 108.1047\n",
      "Epoch: 76, Train Loss: 120.6399, Valid Loss: 107.7930\n",
      "Epoch: 77, Train Loss: 105.1314, Valid Loss: 108.6896\n",
      "Epoch: 78, Train Loss: 110.9904, Valid Loss: 108.8777\n",
      "Epoch: 79, Train Loss: 107.5861, Valid Loss: 103.9941\n",
      "Epoch: 80, Train Loss: 102.2374, Valid Loss: 103.0655\n",
      "Epoch: 81, Train Loss: 103.0423, Valid Loss: 105.0715\n",
      "Epoch: 82, Train Loss: 102.0056, Valid Loss: 101.6697\n",
      "Epoch: 83, Train Loss: 102.1598, Valid Loss: 117.5409\n",
      "Epoch: 84, Train Loss: 98.7137, Valid Loss: 109.6896\n",
      "Epoch: 85, Train Loss: 98.9684, Valid Loss: 110.0566\n",
      "Epoch: 86, Train Loss: 100.3878, Valid Loss: 120.3840\n",
      "Epoch: 87, Train Loss: 118.6810, Valid Loss: 112.3922\n",
      "Epoch: 88, Train Loss: 101.8308, Valid Loss: 101.6632\n",
      "Epoch: 89, Train Loss: 101.0602, Valid Loss: 102.2746\n",
      "Epoch: 90, Train Loss: 96.8628, Valid Loss: 102.3907\n",
      "Epoch: 91, Train Loss: 100.5931, Valid Loss: 105.3550\n",
      "Epoch: 92, Train Loss: 95.7032, Valid Loss: 109.2357\n",
      "Epoch: 93, Train Loss: 94.7800, Valid Loss: 100.1182\n",
      "Epoch: 94, Train Loss: 96.2856, Valid Loss: 98.2112\n",
      "Epoch: 95, Train Loss: 97.3262, Valid Loss: 123.5516\n",
      "Epoch: 96, Train Loss: 104.0713, Valid Loss: 100.5967\n",
      "Epoch: 97, Train Loss: 93.9745, Valid Loss: 109.2663\n",
      "Epoch: 98, Train Loss: 97.1074, Valid Loss: 93.1623\n",
      "Epoch: 99, Train Loss: 108.8644, Valid Loss: 103.0556\n",
      "Epoch: 100, Train Loss: 95.7043, Valid Loss: 93.8124\n",
      "Epoch: 101, Train Loss: 94.8272, Valid Loss: 95.3750\n",
      "Epoch: 102, Train Loss: 93.4112, Valid Loss: 94.8573\n",
      "Epoch: 103, Train Loss: 89.9229, Valid Loss: 101.1272\n",
      "Epoch: 104, Train Loss: 91.8885, Valid Loss: 94.5724\n",
      "Epoch: 105, Train Loss: 89.4383, Valid Loss: 94.5711\n",
      "Epoch: 106, Train Loss: 88.9888, Valid Loss: 93.3096\n",
      "Epoch: 107, Train Loss: 91.9910, Valid Loss: 93.5183\n",
      "Epoch: 108, Train Loss: 89.9701, Valid Loss: 91.1254\n",
      "Epoch: 109, Train Loss: 99.2714, Valid Loss: 93.5577\n",
      "Epoch: 110, Train Loss: 90.4297, Valid Loss: 97.6229\n",
      "Epoch: 111, Train Loss: 89.7432, Valid Loss: 104.7483\n",
      "Epoch: 112, Train Loss: 93.9122, Valid Loss: 95.3626\n",
      "Epoch: 113, Train Loss: 88.9078, Valid Loss: 91.1109\n",
      "Epoch: 114, Train Loss: 86.7549, Valid Loss: 98.3400\n",
      "Epoch: 115, Train Loss: 87.9666, Valid Loss: 88.7154\n",
      "Epoch: 116, Train Loss: 84.8503, Valid Loss: 90.2996\n",
      "Epoch: 117, Train Loss: 85.2177, Valid Loss: 91.7103\n",
      "Epoch: 118, Train Loss: 85.1132, Valid Loss: 85.9055\n",
      "Epoch: 119, Train Loss: 90.9986, Valid Loss: 94.3151\n",
      "Epoch: 120, Train Loss: 84.2941, Valid Loss: 91.4989\n",
      "Epoch: 121, Train Loss: 83.9348, Valid Loss: 90.4227\n",
      "Epoch: 122, Train Loss: 86.1632, Valid Loss: 95.4688\n",
      "Epoch: 123, Train Loss: 87.0124, Valid Loss: 100.6506\n",
      "Epoch: 124, Train Loss: 83.0593, Valid Loss: 92.9104\n",
      "Epoch: 125, Train Loss: 88.0281, Valid Loss: 93.9485\n",
      "Epoch: 126, Train Loss: 87.1310, Valid Loss: 91.5214\n",
      "Epoch: 127, Train Loss: 98.7516, Valid Loss: 107.0314\n",
      "Epoch: 128, Train Loss: 85.3671, Valid Loss: 88.5284\n",
      "Epoch: 129, Train Loss: 82.3357, Valid Loss: 87.9787\n",
      "Epoch: 130, Train Loss: 82.1124, Valid Loss: 87.6347\n",
      "Epoch: 131, Train Loss: 82.1545, Valid Loss: 94.0399\n",
      "Epoch: 132, Train Loss: 81.5908, Valid Loss: 86.7574\n",
      "Epoch: 133, Train Loss: 83.7776, Valid Loss: 83.3529\n",
      "Epoch: 134, Train Loss: 80.4934, Valid Loss: 85.8368\n",
      "Epoch: 135, Train Loss: 79.2174, Valid Loss: 84.3481\n",
      "Epoch: 136, Train Loss: 80.8276, Valid Loss: 99.1204\n",
      "Epoch: 137, Train Loss: 86.0976, Valid Loss: 98.4326\n",
      "Epoch: 138, Train Loss: 85.8367, Valid Loss: 87.9568\n",
      "Epoch: 139, Train Loss: 79.6024, Valid Loss: 86.9667\n",
      "Epoch: 140, Train Loss: 77.8099, Valid Loss: 85.7028\n",
      "Epoch: 141, Train Loss: 76.1350, Valid Loss: 89.1069\n",
      "Epoch: 142, Train Loss: 80.0001, Valid Loss: 87.6579\n",
      "Epoch: 143, Train Loss: 77.7823, Valid Loss: 93.9209\n",
      "Epoch: 144, Train Loss: 78.7301, Valid Loss: 86.6510\n",
      "Epoch: 145, Train Loss: 78.1204, Valid Loss: 90.8144\n",
      "Epoch: 146, Train Loss: 76.6240, Valid Loss: 87.6880\n",
      "Epoch: 147, Train Loss: 79.3204, Valid Loss: 82.5881\n",
      "Epoch: 148, Train Loss: 79.4258, Valid Loss: 86.2236\n",
      "Epoch: 149, Train Loss: 76.1487, Valid Loss: 82.8249\n",
      "Epoch: 150, Train Loss: 75.8704, Valid Loss: 82.1466\n",
      "Epoch: 151, Train Loss: 75.9353, Valid Loss: 81.5741\n",
      "Epoch: 152, Train Loss: 122.8046, Valid Loss: 100.8186\n",
      "Epoch: 153, Train Loss: 85.8628, Valid Loss: 87.4405\n",
      "Epoch: 154, Train Loss: 78.1108, Valid Loss: 84.7084\n",
      "Epoch: 155, Train Loss: 79.0498, Valid Loss: 85.7115\n",
      "Epoch: 156, Train Loss: 92.1061, Valid Loss: 86.5198\n",
      "Epoch: 157, Train Loss: 76.8524, Valid Loss: 83.9445\n",
      "Epoch: 158, Train Loss: 74.9223, Valid Loss: 81.2242\n",
      "Epoch: 159, Train Loss: 74.7559, Valid Loss: 81.6422\n",
      "Epoch: 160, Train Loss: 72.7392, Valid Loss: 87.9012\n",
      "Epoch: 161, Train Loss: 73.3377, Valid Loss: 79.5542\n",
      "Epoch: 162, Train Loss: 74.5275, Valid Loss: 85.9970\n",
      "Epoch: 163, Train Loss: 74.6313, Valid Loss: 77.7426\n",
      "Epoch: 164, Train Loss: 74.0847, Valid Loss: 77.8189\n",
      "Epoch: 165, Train Loss: 73.9659, Valid Loss: 94.0717\n",
      "Epoch: 166, Train Loss: 72.3085, Valid Loss: 80.0049\n",
      "Epoch: 167, Train Loss: 71.9182, Valid Loss: 86.5219\n",
      "Epoch: 168, Train Loss: 73.0663, Valid Loss: 79.5046\n",
      "Epoch: 169, Train Loss: 72.6444, Valid Loss: 80.3851\n",
      "Epoch: 170, Train Loss: 70.2392, Valid Loss: 79.0854\n",
      "Epoch: 171, Train Loss: 74.1864, Valid Loss: 76.4495\n",
      "Epoch: 172, Train Loss: 71.7288, Valid Loss: 85.8079\n",
      "Epoch: 173, Train Loss: 71.2398, Valid Loss: 81.9908\n",
      "Epoch: 174, Train Loss: 72.1177, Valid Loss: 80.7873\n",
      "Epoch: 175, Train Loss: 72.9586, Valid Loss: 86.4923\n",
      "Epoch: 176, Train Loss: 72.6697, Valid Loss: 78.8057\n",
      "Epoch: 177, Train Loss: 71.3752, Valid Loss: 81.0386\n",
      "Epoch: 178, Train Loss: 74.4283, Valid Loss: 91.6462\n",
      "Epoch: 179, Train Loss: 71.2579, Valid Loss: 77.1776\n",
      "Epoch: 180, Train Loss: 70.9644, Valid Loss: 76.5412\n",
      "Epoch: 181, Train Loss: 70.9629, Valid Loss: 82.6819\n",
      "Epoch: 182, Train Loss: 71.6119, Valid Loss: 81.8457\n",
      "Epoch: 183, Train Loss: 73.8846, Valid Loss: 97.5084\n",
      "Epoch: 184, Train Loss: 72.0894, Valid Loss: 92.8505\n",
      "Epoch: 185, Train Loss: 86.6836, Valid Loss: 85.1328\n",
      "Epoch: 186, Train Loss: 71.6891, Valid Loss: 78.3026\n",
      "Epoch: 187, Train Loss: 69.5303, Valid Loss: 80.1964\n",
      "Epoch: 188, Train Loss: 68.4522, Valid Loss: 75.1015\n",
      "Epoch: 189, Train Loss: 71.0676, Valid Loss: 77.4979\n",
      "Epoch: 190, Train Loss: 69.6567, Valid Loss: 96.3876\n",
      "Epoch: 191, Train Loss: 68.6240, Valid Loss: 80.4373\n",
      "Epoch: 192, Train Loss: 70.6129, Valid Loss: 81.5140\n",
      "Epoch: 193, Train Loss: 67.6066, Valid Loss: 76.7535\n",
      "Epoch: 194, Train Loss: 67.7876, Valid Loss: 78.7492\n",
      "Epoch: 195, Train Loss: 68.0047, Valid Loss: 77.2746\n",
      "Epoch: 196, Train Loss: 68.3905, Valid Loss: 75.9841\n",
      "Epoch: 197, Train Loss: 69.1732, Valid Loss: 73.6838\n",
      "Epoch: 198, Train Loss: 84.4776, Valid Loss: 129.3413\n",
      "Epoch: 199, Train Loss: 97.5559, Valid Loss: 102.8168\n",
      "Epoch: 200, Train Loss: 80.5296, Valid Loss: 81.3351\n",
      "Epoch: 201, Train Loss: 71.3286, Valid Loss: 74.8767\n",
      "Epoch: 202, Train Loss: 68.9755, Valid Loss: 82.2399\n",
      "Epoch: 203, Train Loss: 70.9795, Valid Loss: 87.7468\n",
      "Epoch: 204, Train Loss: 67.9353, Valid Loss: 78.6871\n",
      "Epoch: 205, Train Loss: 66.8871, Valid Loss: 75.9393\n",
      "Epoch: 206, Train Loss: 66.4696, Valid Loss: 77.1241\n",
      "Epoch: 207, Train Loss: 66.2130, Valid Loss: 79.4494\n",
      "Epoch: 208, Train Loss: 67.3340, Valid Loss: 78.3368\n",
      "Epoch: 209, Train Loss: 68.0151, Valid Loss: 84.4201\n",
      "Epoch: 210, Train Loss: 67.8081, Valid Loss: 75.0483\n",
      "Epoch: 211, Train Loss: 66.7384, Valid Loss: 75.6713\n",
      "Epoch: 212, Train Loss: 64.5080, Valid Loss: 78.6148\n",
      "Epoch: 213, Train Loss: 65.7626, Valid Loss: 76.9618\n",
      "Epoch: 214, Train Loss: 65.0280, Valid Loss: 80.6417\n",
      "Epoch: 215, Train Loss: 80.1840, Valid Loss: 115.2320\n",
      "Epoch: 216, Train Loss: 82.6983, Valid Loss: 83.8693\n",
      "Epoch: 217, Train Loss: 70.6655, Valid Loss: 81.8807\n",
      "Epoch: 218, Train Loss: 67.9132, Valid Loss: 75.7893\n",
      "Epoch: 219, Train Loss: 68.2564, Valid Loss: 77.6868\n",
      "Epoch: 220, Train Loss: 66.2126, Valid Loss: 72.6159\n",
      "Epoch: 221, Train Loss: 64.2865, Valid Loss: 79.0948\n",
      "Epoch: 222, Train Loss: 64.1113, Valid Loss: 91.0437\n",
      "Epoch: 223, Train Loss: 65.5571, Valid Loss: 73.0864\n",
      "Epoch: 224, Train Loss: 63.1361, Valid Loss: 77.5303\n",
      "Epoch: 225, Train Loss: 63.1946, Valid Loss: 72.2921\n",
      "Epoch: 226, Train Loss: 64.3556, Valid Loss: 75.9270\n",
      "Epoch: 227, Train Loss: 65.8477, Valid Loss: 84.1143\n",
      "Epoch: 228, Train Loss: 70.6029, Valid Loss: 85.1701\n",
      "Epoch: 229, Train Loss: 67.5542, Valid Loss: 75.0152\n",
      "Epoch: 230, Train Loss: 106.9896, Valid Loss: 111.7919\n",
      "Epoch: 231, Train Loss: 77.8553, Valid Loss: 83.9818\n",
      "Epoch: 232, Train Loss: 68.4522, Valid Loss: 74.1819\n",
      "Epoch: 233, Train Loss: 65.1217, Valid Loss: 72.9128\n",
      "Epoch: 234, Train Loss: 64.8057, Valid Loss: 75.4659\n",
      "Epoch: 235, Train Loss: 63.4009, Valid Loss: 75.5536\n",
      "Epoch: 236, Train Loss: 62.5158, Valid Loss: 76.7817\n",
      "Epoch: 237, Train Loss: 61.4674, Valid Loss: 71.4249\n",
      "Epoch: 238, Train Loss: 63.2106, Valid Loss: 75.1702\n",
      "Epoch: 239, Train Loss: 63.6577, Valid Loss: 77.2377\n",
      "Epoch: 240, Train Loss: 62.5564, Valid Loss: 74.8932\n",
      "Epoch: 241, Train Loss: 62.7157, Valid Loss: 80.1653\n",
      "Epoch: 242, Train Loss: 64.0943, Valid Loss: 74.1717\n",
      "Epoch: 243, Train Loss: 62.5499, Valid Loss: 73.7312\n",
      "Epoch: 244, Train Loss: 64.9459, Valid Loss: 75.4163\n",
      "Epoch: 245, Train Loss: 60.8702, Valid Loss: 75.7285\n",
      "Epoch: 246, Train Loss: 62.8810, Valid Loss: 76.4554\n",
      "Epoch: 247, Train Loss: 61.2657, Valid Loss: 70.8546\n",
      "Epoch: 248, Train Loss: 61.0800, Valid Loss: 79.6043\n",
      "Epoch: 249, Train Loss: 62.3030, Valid Loss: 75.6722\n",
      "Epoch: 250, Train Loss: 62.5779, Valid Loss: 73.5728\n",
      "Epoch: 251, Train Loss: 63.7606, Valid Loss: 75.2833\n",
      "Epoch: 252, Train Loss: 64.2960, Valid Loss: 73.5296\n",
      "Epoch: 253, Train Loss: 62.0172, Valid Loss: 74.4935\n",
      "Epoch: 254, Train Loss: 63.5348, Valid Loss: 71.7647\n",
      "Epoch: 255, Train Loss: 62.1669, Valid Loss: 71.3399\n",
      "Epoch: 256, Train Loss: 70.6912, Valid Loss: 103.9724\n",
      "Epoch: 257, Train Loss: 70.7036, Valid Loss: 80.0942\n",
      "Epoch: 258, Train Loss: 62.7338, Valid Loss: 73.8055\n",
      "Epoch: 259, Train Loss: 63.4281, Valid Loss: 72.7146\n",
      "Epoch: 260, Train Loss: 61.2846, Valid Loss: 72.6663\n",
      "Epoch: 261, Train Loss: 60.7335, Valid Loss: 73.4860\n",
      "Epoch: 262, Train Loss: 87.4153, Valid Loss: 127.0854\n",
      "Epoch: 263, Train Loss: 84.5549, Valid Loss: 80.2398\n",
      "Epoch: 264, Train Loss: 66.6258, Valid Loss: 74.8745\n",
      "Epoch: 265, Train Loss: 63.6483, Valid Loss: 85.0261\n",
      "Epoch: 266, Train Loss: 61.8619, Valid Loss: 72.6852\n",
      "Epoch: 267, Train Loss: 61.4827, Valid Loss: 74.2000\n",
      "Epoch: 268, Train Loss: 60.7237, Valid Loss: 75.0785\n",
      "Epoch: 269, Train Loss: 61.0825, Valid Loss: 74.6087\n",
      "Epoch: 270, Train Loss: 60.5924, Valid Loss: 75.4664\n",
      "Epoch: 271, Train Loss: 60.0137, Valid Loss: 71.2879\n",
      "Epoch: 272, Train Loss: 60.5905, Valid Loss: 78.7395\n",
      "Epoch: 273, Train Loss: 61.2134, Valid Loss: 75.8858\n",
      "Epoch: 274, Train Loss: 60.2673, Valid Loss: 70.9779\n",
      "Epoch: 275, Train Loss: 59.4577, Valid Loss: 73.6268\n",
      "Epoch: 276, Train Loss: 59.4701, Valid Loss: 80.0536\n",
      "Epoch: 277, Train Loss: 60.6480, Valid Loss: 84.5360\n",
      "Epoch: 278, Train Loss: 78.0161, Valid Loss: 74.3996\n",
      "Epoch: 279, Train Loss: 74.1881, Valid Loss: 79.0422\n",
      "Epoch: 280, Train Loss: 63.0834, Valid Loss: 72.6135\n",
      "Epoch: 281, Train Loss: 61.4864, Valid Loss: 72.0013\n",
      "Epoch: 282, Train Loss: 64.5526, Valid Loss: 71.2597\n",
      "Epoch: 283, Train Loss: 62.3461, Valid Loss: 71.0700\n",
      "Epoch: 284, Train Loss: 99.0855, Valid Loss: 82.8490\n",
      "Epoch: 285, Train Loss: 72.4537, Valid Loss: 74.1943\n",
      "Epoch: 286, Train Loss: 66.2610, Valid Loss: 74.4316\n",
      "Epoch: 287, Train Loss: 59.5424, Valid Loss: 71.0948\n",
      "Epoch: 288, Train Loss: 59.3528, Valid Loss: 72.4259\n",
      "Epoch: 289, Train Loss: 59.7546, Valid Loss: 70.5549\n",
      "Epoch: 290, Train Loss: 59.1938, Valid Loss: 71.2661\n",
      "Epoch: 291, Train Loss: 114.5356, Valid Loss: 104.1252\n",
      "Epoch: 292, Train Loss: 78.8171, Valid Loss: 82.2943\n",
      "Epoch: 293, Train Loss: 68.0120, Valid Loss: 76.4902\n",
      "Epoch: 294, Train Loss: 64.8421, Valid Loss: 74.3741\n",
      "Epoch: 295, Train Loss: 62.1276, Valid Loss: 78.4006\n",
      "Epoch: 296, Train Loss: 60.5309, Valid Loss: 79.4375\n",
      "Epoch: 297, Train Loss: 60.2870, Valid Loss: 70.7432\n",
      "Epoch: 298, Train Loss: 60.4743, Valid Loss: 79.0237\n",
      "Epoch: 299, Train Loss: 60.4052, Valid Loss: 70.2279\n",
      "Epoch: 300, Train Loss: 59.4013, Valid Loss: 72.9629\n",
      "Epoch: 301, Train Loss: 60.7690, Valid Loss: 80.4594\n",
      "Epoch: 302, Train Loss: 59.1820, Valid Loss: 68.7491\n",
      "Epoch: 303, Train Loss: 59.3715, Valid Loss: 77.7210\n",
      "Epoch: 304, Train Loss: 58.0816, Valid Loss: 71.7880\n",
      "Epoch: 305, Train Loss: 72.1593, Valid Loss: 75.4118\n",
      "Epoch: 306, Train Loss: 60.2835, Valid Loss: 71.4623\n",
      "Epoch: 307, Train Loss: 58.2900, Valid Loss: 77.0398\n",
      "Epoch: 308, Train Loss: 56.2328, Valid Loss: 81.0772\n",
      "Epoch: 309, Train Loss: 62.1555, Valid Loss: 74.5293\n",
      "Epoch: 310, Train Loss: 58.8789, Valid Loss: 69.3257\n",
      "Epoch: 311, Train Loss: 58.0484, Valid Loss: 69.4576\n",
      "Epoch: 312, Train Loss: 58.1758, Valid Loss: 70.6321\n",
      "Epoch: 313, Train Loss: 57.0593, Valid Loss: 72.5069\n",
      "Epoch: 314, Train Loss: 57.5018, Valid Loss: 72.9592\n",
      "Epoch: 315, Train Loss: 59.4564, Valid Loss: 80.8608\n",
      "Epoch: 316, Train Loss: 60.4509, Valid Loss: 71.6277\n",
      "Epoch: 317, Train Loss: 63.6351, Valid Loss: 73.5848\n",
      "Epoch: 318, Train Loss: 58.6774, Valid Loss: 70.6957\n",
      "Epoch: 319, Train Loss: 59.6298, Valid Loss: 68.1678\n",
      "Epoch: 320, Train Loss: 56.5767, Valid Loss: 74.1424\n",
      "Epoch: 321, Train Loss: 57.2231, Valid Loss: 69.9510\n",
      "Epoch: 322, Train Loss: 56.0647, Valid Loss: 68.1103\n",
      "Epoch: 323, Train Loss: 57.8897, Valid Loss: 69.0491\n",
      "Epoch: 324, Train Loss: 77.1682, Valid Loss: 82.6477\n",
      "Epoch: 325, Train Loss: 62.8070, Valid Loss: 74.8636\n",
      "Epoch: 326, Train Loss: 58.0334, Valid Loss: 79.5116\n",
      "Epoch: 327, Train Loss: 59.7938, Valid Loss: 83.9926\n",
      "Epoch: 328, Train Loss: 60.7451, Valid Loss: 89.5222\n",
      "Epoch: 329, Train Loss: 64.8223, Valid Loss: 86.2571\n",
      "Epoch: 330, Train Loss: 59.9522, Valid Loss: 71.5544\n",
      "Epoch: 331, Train Loss: 57.8071, Valid Loss: 71.0908\n",
      "Epoch: 332, Train Loss: 56.5335, Valid Loss: 69.6084\n",
      "Epoch: 333, Train Loss: 56.3584, Valid Loss: 69.3145\n",
      "Epoch: 334, Train Loss: 55.1890, Valid Loss: 67.8867\n",
      "Epoch: 335, Train Loss: 57.2204, Valid Loss: 67.2031\n",
      "Epoch: 336, Train Loss: 55.2884, Valid Loss: 67.9716\n",
      "Epoch: 337, Train Loss: 55.3178, Valid Loss: 68.5129\n",
      "Epoch: 338, Train Loss: 55.4793, Valid Loss: 73.0662\n",
      "Epoch: 339, Train Loss: 56.8540, Valid Loss: 84.6386\n",
      "Epoch: 340, Train Loss: 56.8756, Valid Loss: 74.5899\n",
      "Epoch: 341, Train Loss: 55.5054, Valid Loss: 68.4373\n",
      "Epoch: 342, Train Loss: 55.0277, Valid Loss: 70.7422\n",
      "Epoch: 343, Train Loss: 95.7032, Valid Loss: 88.9366\n",
      "Epoch: 344, Train Loss: 72.4369, Valid Loss: 77.7816\n",
      "Epoch: 345, Train Loss: 60.0055, Valid Loss: 73.0835\n",
      "Epoch: 346, Train Loss: 59.4717, Valid Loss: 70.9485\n",
      "Epoch: 347, Train Loss: 60.4997, Valid Loss: 70.9997\n",
      "Epoch: 348, Train Loss: 56.5863, Valid Loss: 68.9917\n",
      "Epoch: 349, Train Loss: 54.7098, Valid Loss: 69.2590\n",
      "Epoch: 350, Train Loss: 57.6927, Valid Loss: 67.7312\n",
      "Epoch: 351, Train Loss: 55.5603, Valid Loss: 67.2450\n",
      "Epoch: 352, Train Loss: 55.0303, Valid Loss: 73.5429\n",
      "Epoch: 353, Train Loss: 55.0047, Valid Loss: 66.8676\n",
      "Epoch: 354, Train Loss: 54.6051, Valid Loss: 68.6866\n",
      "Epoch: 355, Train Loss: 55.9026, Valid Loss: 68.6670\n",
      "Epoch: 356, Train Loss: 55.6266, Valid Loss: 70.0549\n",
      "Epoch: 357, Train Loss: 52.1505, Valid Loss: 65.6407\n",
      "Epoch: 358, Train Loss: 52.7543, Valid Loss: 68.9711\n",
      "Epoch: 359, Train Loss: 55.6026, Valid Loss: 71.8460\n",
      "Epoch: 360, Train Loss: 58.6843, Valid Loss: 76.7387\n",
      "Epoch: 361, Train Loss: 53.6432, Valid Loss: 66.2641\n",
      "Epoch: 362, Train Loss: 53.3926, Valid Loss: 72.9713\n",
      "Epoch: 363, Train Loss: 54.2330, Valid Loss: 67.0891\n",
      "Epoch: 364, Train Loss: 54.0134, Valid Loss: 68.9038\n",
      "Epoch: 365, Train Loss: 56.6110, Valid Loss: 76.0451\n",
      "Epoch: 366, Train Loss: 55.8364, Valid Loss: 67.8583\n",
      "Epoch: 367, Train Loss: 55.3080, Valid Loss: 73.3754\n",
      "Epoch: 368, Train Loss: 60.9309, Valid Loss: 72.7899\n",
      "Epoch: 369, Train Loss: 56.0744, Valid Loss: 70.3455\n",
      "Epoch: 370, Train Loss: 54.2026, Valid Loss: 69.4434\n",
      "Epoch: 371, Train Loss: 55.9318, Valid Loss: 71.7801\n",
      "Epoch: 372, Train Loss: 54.6568, Valid Loss: 82.0872\n",
      "Epoch: 373, Train Loss: 57.1034, Valid Loss: 69.8665\n",
      "Epoch: 374, Train Loss: 54.1891, Valid Loss: 72.7402\n",
      "Epoch: 375, Train Loss: 55.2330, Valid Loss: 72.2566\n",
      "Epoch: 376, Train Loss: 55.0326, Valid Loss: 69.0872\n",
      "Epoch: 377, Train Loss: 53.1011, Valid Loss: 69.7785\n",
      "Epoch: 378, Train Loss: 53.4362, Valid Loss: 63.2559\n",
      "Epoch: 379, Train Loss: 84.9986, Valid Loss: 89.6215\n",
      "Epoch: 380, Train Loss: 68.4474, Valid Loss: 75.8937\n",
      "Epoch: 381, Train Loss: 57.7576, Valid Loss: 68.8466\n",
      "Epoch: 382, Train Loss: 54.7781, Valid Loss: 66.7715\n",
      "Epoch: 383, Train Loss: 55.1121, Valid Loss: 70.2008\n",
      "Epoch: 384, Train Loss: 61.0727, Valid Loss: 84.6764\n",
      "Epoch: 385, Train Loss: 81.4115, Valid Loss: 131.9870\n",
      "Epoch: 386, Train Loss: 93.4906, Valid Loss: 134.0243\n",
      "Epoch: 387, Train Loss: 72.2273, Valid Loss: 72.5603\n",
      "Epoch: 388, Train Loss: 58.1221, Valid Loss: 72.2125\n",
      "Epoch: 389, Train Loss: 57.8162, Valid Loss: 69.5782\n",
      "Epoch: 390, Train Loss: 55.0032, Valid Loss: 69.1460\n",
      "Epoch: 391, Train Loss: 54.1803, Valid Loss: 70.6900\n",
      "Epoch: 392, Train Loss: 54.6662, Valid Loss: 66.0334\n",
      "Epoch: 393, Train Loss: 52.4941, Valid Loss: 74.0732\n",
      "Epoch: 394, Train Loss: 52.0398, Valid Loss: 78.6155\n",
      "Epoch: 395, Train Loss: 52.7513, Valid Loss: 64.5295\n",
      "Epoch: 396, Train Loss: 54.5481, Valid Loss: 84.4080\n",
      "Epoch: 397, Train Loss: 56.0825, Valid Loss: 72.0227\n",
      "Epoch: 398, Train Loss: 53.5828, Valid Loss: 70.7591\n",
      "Epoch: 399, Train Loss: 53.3538, Valid Loss: 64.9578\n",
      "Epoch: 400, Train Loss: 52.1018, Valid Loss: 62.9192\n",
      "Epoch: 401, Train Loss: 52.3719, Valid Loss: 69.0444\n",
      "Epoch: 402, Train Loss: 50.5204, Valid Loss: 68.2157\n",
      "Epoch: 403, Train Loss: 58.3985, Valid Loss: 69.4481\n",
      "Epoch: 404, Train Loss: 53.8518, Valid Loss: 83.3026\n",
      "Epoch: 405, Train Loss: 53.6328, Valid Loss: 65.6747\n",
      "Epoch: 406, Train Loss: 52.1649, Valid Loss: 65.5758\n",
      "Epoch: 407, Train Loss: 51.6074, Valid Loss: 67.8305\n",
      "Epoch: 408, Train Loss: 51.2562, Valid Loss: 66.8482\n",
      "Epoch: 409, Train Loss: 50.1643, Valid Loss: 64.0073\n",
      "Epoch: 410, Train Loss: 51.4673, Valid Loss: 64.8468\n",
      "Epoch: 411, Train Loss: 50.9758, Valid Loss: 65.0840\n",
      "Epoch: 412, Train Loss: 50.7737, Valid Loss: 68.1497\n",
      "Epoch: 413, Train Loss: 51.7637, Valid Loss: 64.4789\n",
      "Epoch: 414, Train Loss: 53.9755, Valid Loss: 70.7553\n",
      "Epoch: 415, Train Loss: 58.2621, Valid Loss: 68.4232\n",
      "Epoch: 416, Train Loss: 51.7984, Valid Loss: 63.5906\n",
      "Epoch: 417, Train Loss: 51.9269, Valid Loss: 64.4617\n",
      "Epoch: 418, Train Loss: 52.6476, Valid Loss: 66.5502\n",
      "Epoch: 419, Train Loss: 97.9467, Valid Loss: 90.8696\n",
      "Epoch: 420, Train Loss: 82.3892, Valid Loss: 84.8016\n",
      "Epoch: 421, Train Loss: 66.3105, Valid Loss: 80.3867\n",
      "Epoch: 422, Train Loss: 60.4067, Valid Loss: 70.1324\n",
      "Epoch: 423, Train Loss: 55.9972, Valid Loss: 67.5594\n",
      "Epoch: 424, Train Loss: 53.6101, Valid Loss: 69.0767\n",
      "Epoch: 425, Train Loss: 53.2638, Valid Loss: 63.8644\n",
      "Epoch: 426, Train Loss: 55.9152, Valid Loss: 83.2486\n",
      "Epoch: 427, Train Loss: 56.8917, Valid Loss: 70.5466\n",
      "Epoch: 428, Train Loss: 53.6800, Valid Loss: 62.6813\n",
      "Epoch: 429, Train Loss: 50.1757, Valid Loss: 68.3510\n",
      "Epoch: 430, Train Loss: 118.5271, Valid Loss: 118.1942\n",
      "Epoch: 431, Train Loss: 103.1783, Valid Loss: 91.0606\n",
      "Epoch: 432, Train Loss: 83.0523, Valid Loss: 86.9402\n",
      "Epoch: 433, Train Loss: 74.2637, Valid Loss: 84.5923\n",
      "Epoch: 434, Train Loss: 67.9975, Valid Loss: 73.7300\n",
      "Epoch: 435, Train Loss: 64.9626, Valid Loss: 74.2087\n",
      "Epoch: 436, Train Loss: 59.0051, Valid Loss: 70.7328\n",
      "Epoch: 437, Train Loss: 57.9839, Valid Loss: 69.6254\n",
      "Epoch: 438, Train Loss: 54.4320, Valid Loss: 69.2470\n",
      "Epoch: 439, Train Loss: 54.8414, Valid Loss: 66.3146\n",
      "Epoch: 440, Train Loss: 52.3195, Valid Loss: 72.8324\n",
      "Epoch: 441, Train Loss: 52.9081, Valid Loss: 65.0782\n",
      "Epoch: 442, Train Loss: 56.9693, Valid Loss: 69.7264\n",
      "Epoch: 443, Train Loss: 56.7306, Valid Loss: 68.2947\n",
      "Epoch: 444, Train Loss: 53.7849, Valid Loss: 68.6230\n",
      "Epoch: 445, Train Loss: 53.6668, Valid Loss: 70.4273\n",
      "Epoch: 446, Train Loss: 51.8877, Valid Loss: 66.3916\n",
      "Epoch: 447, Train Loss: 51.3116, Valid Loss: 63.9784\n",
      "Epoch: 448, Train Loss: 56.3743, Valid Loss: 80.7996\n",
      "Epoch: 449, Train Loss: 58.7028, Valid Loss: 65.8788\n",
      "Epoch: 450, Train Loss: 53.0966, Valid Loss: 67.5283\n",
      "Epoch: 451, Train Loss: 52.1008, Valid Loss: 63.9709\n",
      "Epoch: 452, Train Loss: 51.3051, Valid Loss: 67.3162\n",
      "Epoch: 453, Train Loss: 56.1267, Valid Loss: 65.6671\n",
      "Epoch: 454, Train Loss: 63.7051, Valid Loss: 67.2183\n",
      "Epoch: 455, Train Loss: 53.4310, Valid Loss: 69.1134\n",
      "Epoch: 456, Train Loss: 53.4456, Valid Loss: 67.6531\n",
      "Epoch: 457, Train Loss: 51.7775, Valid Loss: 69.2346\n",
      "Epoch: 458, Train Loss: 55.6953, Valid Loss: 72.0252\n",
      "Epoch: 459, Train Loss: 56.5019, Valid Loss: 68.0353\n",
      "Epoch: 460, Train Loss: 50.9454, Valid Loss: 65.3071\n",
      "Epoch: 461, Train Loss: 50.2336, Valid Loss: 66.5652\n",
      "Epoch: 462, Train Loss: 50.0490, Valid Loss: 69.9283\n",
      "Epoch: 463, Train Loss: 51.1452, Valid Loss: 67.3350\n",
      "Epoch: 464, Train Loss: 50.5033, Valid Loss: 65.5497\n",
      "Epoch: 465, Train Loss: 50.3709, Valid Loss: 96.9118\n",
      "Epoch: 466, Train Loss: 54.4836, Valid Loss: 67.0982\n",
      "Epoch: 467, Train Loss: 52.4424, Valid Loss: 63.3426\n",
      "Epoch: 468, Train Loss: 49.6794, Valid Loss: 63.5816\n",
      "Epoch: 469, Train Loss: 50.7671, Valid Loss: 65.8447\n",
      "Epoch: 470, Train Loss: 49.6985, Valid Loss: 65.2863\n",
      "Epoch: 471, Train Loss: 49.2363, Valid Loss: 64.0905\n",
      "Epoch: 472, Train Loss: 49.5924, Valid Loss: 66.2727\n",
      "Epoch: 473, Train Loss: 48.7021, Valid Loss: 64.0709\n",
      "Epoch: 474, Train Loss: 48.9201, Valid Loss: 61.8706\n",
      "Epoch: 475, Train Loss: 60.4899, Valid Loss: 69.8392\n",
      "Epoch: 476, Train Loss: 52.2716, Valid Loss: 65.1366\n",
      "Epoch: 477, Train Loss: 49.1438, Valid Loss: 60.8588\n",
      "Epoch: 478, Train Loss: 52.1741, Valid Loss: 64.1223\n",
      "Epoch: 479, Train Loss: 52.0146, Valid Loss: 66.0927\n",
      "Epoch: 480, Train Loss: 50.8258, Valid Loss: 64.7420\n",
      "Epoch: 481, Train Loss: 49.1296, Valid Loss: 70.4397\n",
      "Epoch: 482, Train Loss: 50.3386, Valid Loss: 63.8097\n",
      "Epoch: 483, Train Loss: 50.1665, Valid Loss: 63.2612\n",
      "Epoch: 484, Train Loss: 49.5235, Valid Loss: 63.1999\n",
      "Epoch: 485, Train Loss: 50.5035, Valid Loss: 65.6265\n",
      "Epoch: 486, Train Loss: 57.4419, Valid Loss: 72.8421\n",
      "Epoch: 487, Train Loss: 54.2139, Valid Loss: 67.3655\n",
      "Epoch: 488, Train Loss: 52.8122, Valid Loss: 66.2514\n",
      "Epoch: 489, Train Loss: 59.0299, Valid Loss: 72.7496\n",
      "Epoch: 490, Train Loss: 50.6690, Valid Loss: 61.1160\n",
      "Epoch: 491, Train Loss: 47.0697, Valid Loss: 60.8593\n",
      "Epoch: 492, Train Loss: 50.2116, Valid Loss: 91.9979\n",
      "Epoch: 493, Train Loss: 50.9718, Valid Loss: 61.4982\n",
      "Epoch: 494, Train Loss: 50.3041, Valid Loss: 62.3821\n",
      "Epoch: 495, Train Loss: 47.8009, Valid Loss: 61.2147\n",
      "Epoch: 496, Train Loss: 47.3714, Valid Loss: 60.8845\n",
      "Epoch: 497, Train Loss: 47.2408, Valid Loss: 63.2958\n",
      "Epoch: 498, Train Loss: 46.5245, Valid Loss: 62.3635\n",
      "Epoch: 499, Train Loss: 75.0054, Valid Loss: 133.7025\n",
      "Epoch: 500, Train Loss: 140.4024, Valid Loss: 105.3369\n",
      "Epoch: 501, Train Loss: 87.6861, Valid Loss: 85.0373\n",
      "Epoch: 502, Train Loss: 72.7726, Valid Loss: 78.8108\n",
      "Epoch: 503, Train Loss: 64.9063, Valid Loss: 77.8887\n",
      "Epoch: 504, Train Loss: 60.8514, Valid Loss: 75.1446\n",
      "Epoch: 505, Train Loss: 56.7172, Valid Loss: 70.1400\n",
      "Epoch: 506, Train Loss: 55.9172, Valid Loss: 71.7912\n",
      "Epoch: 507, Train Loss: 52.5765, Valid Loss: 66.4369\n",
      "Epoch: 508, Train Loss: 52.4488, Valid Loss: 70.6677\n",
      "Epoch: 509, Train Loss: 53.2793, Valid Loss: 65.8281\n",
      "Epoch: 510, Train Loss: 49.6348, Valid Loss: 64.2957\n",
      "Epoch: 511, Train Loss: 49.3352, Valid Loss: 65.8387\n",
      "Epoch: 512, Train Loss: 49.9584, Valid Loss: 64.2390\n",
      "Epoch: 513, Train Loss: 47.5236, Valid Loss: 65.7803\n",
      "Epoch: 514, Train Loss: 48.1321, Valid Loss: 66.7600\n",
      "Epoch: 515, Train Loss: 47.2913, Valid Loss: 62.1906\n",
      "Epoch: 516, Train Loss: 49.0854, Valid Loss: 63.4811\n",
      "Epoch: 517, Train Loss: 47.2409, Valid Loss: 66.4419\n",
      "Epoch: 518, Train Loss: 46.9823, Valid Loss: 67.4460\n",
      "Epoch: 519, Train Loss: 55.1011, Valid Loss: 68.3692\n",
      "Epoch: 520, Train Loss: 50.3394, Valid Loss: 70.5555\n",
      "Epoch: 521, Train Loss: 48.4114, Valid Loss: 63.7786\n",
      "Epoch: 522, Train Loss: 48.4096, Valid Loss: 60.3344\n",
      "Epoch: 523, Train Loss: 46.4880, Valid Loss: 63.6937\n",
      "Epoch: 524, Train Loss: 46.5961, Valid Loss: 61.3307\n",
      "Epoch: 525, Train Loss: 56.5248, Valid Loss: 63.0950\n",
      "Epoch: 526, Train Loss: 47.2757, Valid Loss: 64.3725\n",
      "Epoch: 527, Train Loss: 48.3972, Valid Loss: 64.2857\n",
      "Epoch: 528, Train Loss: 46.6493, Valid Loss: 62.8609\n",
      "Epoch: 529, Train Loss: 48.8148, Valid Loss: 71.6951\n",
      "Epoch: 530, Train Loss: 49.0243, Valid Loss: 59.8890\n",
      "Epoch: 531, Train Loss: 47.8985, Valid Loss: 66.4943\n",
      "Epoch: 532, Train Loss: 47.5889, Valid Loss: 65.9385\n",
      "Epoch: 533, Train Loss: 47.3561, Valid Loss: 61.6138\n",
      "Epoch: 534, Train Loss: 48.8488, Valid Loss: 63.2280\n",
      "Epoch: 535, Train Loss: 47.4042, Valid Loss: 75.5707\n",
      "Epoch: 536, Train Loss: 49.7124, Valid Loss: 62.5740\n",
      "Epoch: 537, Train Loss: 46.7881, Valid Loss: 65.0434\n",
      "Epoch: 538, Train Loss: 47.5231, Valid Loss: 63.5561\n",
      "Epoch: 539, Train Loss: 47.0817, Valid Loss: 58.9386\n",
      "Epoch: 540, Train Loss: 51.3283, Valid Loss: 75.6393\n",
      "Epoch: 541, Train Loss: 49.7140, Valid Loss: 64.0233\n",
      "Epoch: 542, Train Loss: 46.6826, Valid Loss: 62.8210\n",
      "Epoch: 543, Train Loss: 47.5633, Valid Loss: 61.0097\n",
      "Epoch: 544, Train Loss: 47.6987, Valid Loss: 63.9917\n",
      "Epoch: 545, Train Loss: 48.5041, Valid Loss: 69.5912\n",
      "Epoch: 546, Train Loss: 61.0566, Valid Loss: 65.3128\n",
      "Epoch: 547, Train Loss: 49.5176, Valid Loss: 64.8510\n",
      "Epoch: 548, Train Loss: 47.0949, Valid Loss: 68.2744\n",
      "Epoch: 549, Train Loss: 54.8628, Valid Loss: 66.8720\n",
      "Epoch: 550, Train Loss: 49.0340, Valid Loss: 61.5099\n",
      "Epoch: 551, Train Loss: 46.4842, Valid Loss: 63.7535\n",
      "Epoch: 552, Train Loss: 46.8647, Valid Loss: 63.5871\n",
      "Epoch: 553, Train Loss: 49.2731, Valid Loss: 65.9915\n",
      "Epoch: 554, Train Loss: 48.2679, Valid Loss: 77.1836\n",
      "Epoch: 555, Train Loss: 49.2115, Valid Loss: 68.0375\n",
      "Epoch: 556, Train Loss: 51.6021, Valid Loss: 66.1510\n",
      "Epoch: 557, Train Loss: 47.9869, Valid Loss: 63.9754\n",
      "Epoch: 558, Train Loss: 45.6707, Valid Loss: 62.8779\n",
      "Epoch: 559, Train Loss: 57.7493, Valid Loss: 78.9875\n",
      "Epoch: 560, Train Loss: 51.1686, Valid Loss: 64.6065\n",
      "Epoch: 561, Train Loss: 46.4982, Valid Loss: 63.3627\n",
      "Epoch: 562, Train Loss: 46.2657, Valid Loss: 60.7516\n",
      "Epoch: 563, Train Loss: 45.1133, Valid Loss: 61.9430\n",
      "Epoch: 564, Train Loss: 49.3537, Valid Loss: 64.7061\n",
      "Epoch: 565, Train Loss: 46.5844, Valid Loss: 63.8897\n",
      "Epoch: 566, Train Loss: 46.8802, Valid Loss: 62.9974\n",
      "Epoch: 567, Train Loss: 45.7863, Valid Loss: 62.7859\n",
      "Epoch: 568, Train Loss: 46.2046, Valid Loss: 62.3882\n",
      "Epoch: 569, Train Loss: 44.9639, Valid Loss: 63.7648\n",
      "Epoch: 570, Train Loss: 44.0276, Valid Loss: 62.5387\n",
      "Epoch: 571, Train Loss: 45.5744, Valid Loss: 61.7585\n",
      "Epoch: 572, Train Loss: 47.5316, Valid Loss: 61.6119\n",
      "Epoch: 573, Train Loss: 57.9616, Valid Loss: 66.8641\n",
      "Epoch: 574, Train Loss: 46.0716, Valid Loss: 61.9579\n",
      "Epoch: 575, Train Loss: 45.0930, Valid Loss: 60.9363\n",
      "Epoch: 576, Train Loss: 49.5621, Valid Loss: 80.9591\n",
      "Epoch: 577, Train Loss: 51.6164, Valid Loss: 65.9785\n",
      "Epoch: 578, Train Loss: 49.1021, Valid Loss: 63.4511\n",
      "Epoch: 579, Train Loss: 45.9904, Valid Loss: 67.2469\n",
      "Epoch: 580, Train Loss: 71.6783, Valid Loss: 89.2914\n",
      "Epoch: 581, Train Loss: 66.1444, Valid Loss: 65.9438\n",
      "Epoch: 582, Train Loss: 55.6250, Valid Loss: 64.3934\n",
      "Epoch: 583, Train Loss: 48.5977, Valid Loss: 64.1166\n",
      "Epoch: 584, Train Loss: 48.2997, Valid Loss: 62.9803\n",
      "Epoch: 585, Train Loss: 51.3214, Valid Loss: 66.9650\n",
      "Epoch: 586, Train Loss: 48.3936, Valid Loss: 63.3937\n",
      "Epoch: 587, Train Loss: 46.0131, Valid Loss: 62.8758\n",
      "Epoch: 588, Train Loss: 45.5083, Valid Loss: 63.2046\n",
      "Epoch: 589, Train Loss: 46.8107, Valid Loss: 63.9055\n",
      "Learning rate has been changed to: 0.0001\n",
      "Epoch: 590, Train Loss: 48.2416, Valid Loss: 66.5986\n",
      "Epoch: 591, Train Loss: 41.6439, Valid Loss: 59.8846\n",
      "Epoch: 592, Train Loss: 41.0567, Valid Loss: 58.5080\n",
      "Epoch: 593, Train Loss: 40.1085, Valid Loss: 59.3009\n",
      "Epoch: 594, Train Loss: 41.1017, Valid Loss: 57.1852\n",
      "Epoch: 595, Train Loss: 40.6420, Valid Loss: 57.0933\n",
      "Epoch: 596, Train Loss: 39.5467, Valid Loss: 58.0198\n",
      "Epoch: 597, Train Loss: 39.7518, Valid Loss: 56.4085\n",
      "Epoch: 598, Train Loss: 39.5714, Valid Loss: 56.8733\n",
      "Epoch: 599, Train Loss: 39.1641, Valid Loss: 57.7557\n",
      "Epoch: 600, Train Loss: 39.3539, Valid Loss: 59.1051\n",
      "Epoch: 601, Train Loss: 39.4149, Valid Loss: 59.7179\n",
      "Epoch: 602, Train Loss: 38.8680, Valid Loss: 56.2871\n",
      "Epoch: 603, Train Loss: 39.2163, Valid Loss: 57.3916\n",
      "Epoch: 604, Train Loss: 39.2712, Valid Loss: 56.7674\n",
      "Epoch: 605, Train Loss: 39.2471, Valid Loss: 56.6453\n",
      "Epoch: 606, Train Loss: 39.0745, Valid Loss: 56.9386\n",
      "Epoch: 607, Train Loss: 38.8980, Valid Loss: 58.8899\n",
      "Epoch: 608, Train Loss: 39.1440, Valid Loss: 58.1136\n",
      "Epoch: 609, Train Loss: 38.7604, Valid Loss: 57.6431\n",
      "Epoch: 610, Train Loss: 38.6283, Valid Loss: 58.8105\n",
      "Epoch: 611, Train Loss: 38.6304, Valid Loss: 56.0162\n",
      "Epoch: 612, Train Loss: 38.5326, Valid Loss: 58.1570\n",
      "Epoch: 613, Train Loss: 38.4898, Valid Loss: 56.3867\n",
      "Epoch: 614, Train Loss: 38.4170, Valid Loss: 57.7037\n",
      "Epoch: 615, Train Loss: 38.5380, Valid Loss: 57.3309\n",
      "Epoch: 616, Train Loss: 38.4651, Valid Loss: 55.9459\n",
      "Epoch: 617, Train Loss: 38.6611, Valid Loss: 55.0373\n",
      "Epoch: 618, Train Loss: 38.4989, Valid Loss: 56.1231\n",
      "Epoch: 619, Train Loss: 38.6343, Valid Loss: 57.9132\n",
      "Epoch: 620, Train Loss: 38.4845, Valid Loss: 59.4192\n",
      "Epoch: 621, Train Loss: 38.5826, Valid Loss: 56.9383\n",
      "Epoch: 622, Train Loss: 38.5666, Valid Loss: 55.6311\n",
      "Epoch: 623, Train Loss: 38.0784, Valid Loss: 57.9057\n",
      "Epoch: 624, Train Loss: 37.8581, Valid Loss: 57.3126\n",
      "Epoch: 625, Train Loss: 38.0488, Valid Loss: 56.8342\n",
      "Epoch: 626, Train Loss: 38.2249, Valid Loss: 56.0130\n",
      "Epoch: 627, Train Loss: 38.2183, Valid Loss: 59.2988\n",
      "Epoch: 628, Train Loss: 37.7275, Valid Loss: 57.2527\n",
      "Epoch: 629, Train Loss: 38.2119, Valid Loss: 55.6838\n",
      "Epoch: 630, Train Loss: 38.1810, Valid Loss: 56.3865\n",
      "Epoch: 631, Train Loss: 37.8038, Valid Loss: 57.7422\n",
      "Epoch: 632, Train Loss: 37.6556, Valid Loss: 57.6979\n",
      "Epoch: 633, Train Loss: 38.1915, Valid Loss: 55.8084\n",
      "Epoch: 634, Train Loss: 37.8116, Valid Loss: 55.8132\n",
      "Epoch: 635, Train Loss: 37.5993, Valid Loss: 56.8970\n",
      "Epoch: 636, Train Loss: 37.9584, Valid Loss: 55.8747\n",
      "Epoch: 637, Train Loss: 37.2929, Valid Loss: 58.0842\n",
      "Epoch: 638, Train Loss: 37.3094, Valid Loss: 56.8794\n",
      "Epoch: 639, Train Loss: 37.6905, Valid Loss: 55.7986\n",
      "Epoch: 640, Train Loss: 37.4353, Valid Loss: 55.3478\n",
      "Epoch: 641, Train Loss: 37.7058, Valid Loss: 55.3743\n",
      "Epoch: 642, Train Loss: 37.2385, Valid Loss: 55.5495\n",
      "Epoch: 643, Train Loss: 36.9410, Valid Loss: 57.6487\n",
      "Epoch: 644, Train Loss: 37.5777, Valid Loss: 55.7456\n",
      "Epoch: 645, Train Loss: 37.1214, Valid Loss: 55.7682\n",
      "Epoch: 646, Train Loss: 37.5148, Valid Loss: 56.0288\n",
      "Epoch: 647, Train Loss: 37.0899, Valid Loss: 56.8591\n",
      "Epoch: 648, Train Loss: 37.5837, Valid Loss: 57.4515\n",
      "Epoch: 649, Train Loss: 37.3882, Valid Loss: 58.6837\n",
      "Epoch: 650, Train Loss: 37.0414, Valid Loss: 58.1133\n",
      "Epoch: 651, Train Loss: 40.5731, Valid Loss: 55.6824\n",
      "Epoch: 652, Train Loss: 37.5757, Valid Loss: 56.3032\n",
      "Epoch: 653, Train Loss: 37.0250, Valid Loss: 55.2125\n",
      "Epoch: 654, Train Loss: 37.0448, Valid Loss: 56.2741\n",
      "Epoch: 655, Train Loss: 37.3818, Valid Loss: 57.9876\n",
      "Epoch: 656, Train Loss: 36.7107, Valid Loss: 55.1991\n",
      "Epoch: 657, Train Loss: 36.5214, Valid Loss: 57.1859\n",
      "Epoch: 658, Train Loss: 37.1506, Valid Loss: 58.9258\n",
      "Epoch: 659, Train Loss: 37.3962, Valid Loss: 57.8170\n",
      "Epoch: 660, Train Loss: 37.0004, Valid Loss: 56.2131\n",
      "Epoch: 661, Train Loss: 37.5301, Valid Loss: 54.9077\n",
      "Epoch: 662, Train Loss: 36.9801, Valid Loss: 56.5107\n",
      "Epoch: 663, Train Loss: 36.5479, Valid Loss: 57.4695\n",
      "Epoch: 664, Train Loss: 36.6188, Valid Loss: 56.3181\n",
      "Epoch: 665, Train Loss: 36.7336, Valid Loss: 54.5868\n",
      "Epoch: 666, Train Loss: 36.9577, Valid Loss: 53.7941\n",
      "Epoch: 667, Train Loss: 36.5576, Valid Loss: 55.5335\n",
      "Epoch: 668, Train Loss: 36.6869, Valid Loss: 57.3153\n",
      "Epoch: 669, Train Loss: 37.0270, Valid Loss: 56.0489\n",
      "Epoch: 670, Train Loss: 36.6301, Valid Loss: 58.6100\n",
      "Epoch: 671, Train Loss: 36.9097, Valid Loss: 55.3294\n",
      "Epoch: 672, Train Loss: 36.8552, Valid Loss: 56.5865\n",
      "Epoch: 673, Train Loss: 36.4680, Valid Loss: 52.9918\n",
      "Epoch: 674, Train Loss: 40.0922, Valid Loss: 59.5439\n",
      "Epoch: 675, Train Loss: 37.8342, Valid Loss: 56.6325\n",
      "Epoch: 676, Train Loss: 36.7884, Valid Loss: 55.8789\n",
      "Epoch: 677, Train Loss: 37.2510, Valid Loss: 58.6457\n",
      "Epoch: 678, Train Loss: 36.8389, Valid Loss: 54.4077\n",
      "Epoch: 679, Train Loss: 36.5247, Valid Loss: 56.7479\n",
      "Epoch: 680, Train Loss: 36.6044, Valid Loss: 54.2737\n",
      "Epoch: 681, Train Loss: 37.0384, Valid Loss: 55.8951\n",
      "Epoch: 682, Train Loss: 36.2977, Valid Loss: 57.2355\n",
      "Epoch: 683, Train Loss: 36.7861, Valid Loss: 55.4006\n",
      "Epoch: 684, Train Loss: 36.1688, Valid Loss: 58.3718\n",
      "Epoch: 685, Train Loss: 36.7231, Valid Loss: 55.3996\n",
      "Epoch: 686, Train Loss: 36.7191, Valid Loss: 57.2274\n",
      "Epoch: 687, Train Loss: 36.4588, Valid Loss: 55.8945\n",
      "Epoch: 688, Train Loss: 36.5586, Valid Loss: 54.5975\n",
      "Epoch: 689, Train Loss: 36.1419, Valid Loss: 55.3994\n",
      "Epoch: 690, Train Loss: 36.5328, Valid Loss: 55.5391\n",
      "Epoch: 691, Train Loss: 36.5060, Valid Loss: 58.8392\n",
      "Epoch: 692, Train Loss: 36.1575, Valid Loss: 57.7629\n",
      "Epoch: 693, Train Loss: 36.5200, Valid Loss: 57.0633\n",
      "Epoch: 694, Train Loss: 36.6533, Valid Loss: 56.2907\n",
      "Epoch: 695, Train Loss: 36.4481, Valid Loss: 57.0578\n",
      "Epoch: 696, Train Loss: 36.6462, Valid Loss: 56.8359\n",
      "Epoch: 697, Train Loss: 36.3601, Valid Loss: 57.3620\n",
      "Epoch: 698, Train Loss: 36.2276, Valid Loss: 57.9604\n",
      "Epoch: 699, Train Loss: 36.3459, Valid Loss: 55.3570\n",
      "Epoch: 700, Train Loss: 35.9371, Valid Loss: 55.8994\n",
      "Epoch: 701, Train Loss: 35.9778, Valid Loss: 58.2266\n",
      "Epoch: 702, Train Loss: 36.5479, Valid Loss: 56.6138\n",
      "Epoch: 703, Train Loss: 36.0693, Valid Loss: 56.1260\n",
      "Epoch: 704, Train Loss: 36.5040, Valid Loss: 58.7224\n",
      "Epoch: 705, Train Loss: 36.8230, Valid Loss: 54.8530\n",
      "Epoch: 706, Train Loss: 35.9308, Valid Loss: 54.7768\n",
      "Epoch: 707, Train Loss: 36.1677, Valid Loss: 56.4761\n",
      "Epoch: 708, Train Loss: 36.0024, Valid Loss: 53.7562\n",
      "Epoch: 709, Train Loss: 36.4024, Valid Loss: 55.1935\n",
      "Epoch: 710, Train Loss: 36.3447, Valid Loss: 56.1163\n",
      "Epoch: 711, Train Loss: 36.2612, Valid Loss: 56.1881\n",
      "Epoch: 712, Train Loss: 36.0953, Valid Loss: 55.6137\n",
      "Epoch: 713, Train Loss: 35.9932, Valid Loss: 54.2002\n",
      "Epoch: 714, Train Loss: 35.6967, Valid Loss: 54.5705\n",
      "Epoch: 715, Train Loss: 36.1099, Valid Loss: 53.9337\n",
      "Epoch: 716, Train Loss: 36.2507, Valid Loss: 56.0749\n",
      "Epoch: 717, Train Loss: 36.0635, Valid Loss: 57.1518\n",
      "Epoch: 718, Train Loss: 35.8906, Valid Loss: 54.9267\n",
      "Epoch: 719, Train Loss: 36.3148, Valid Loss: 57.3734\n",
      "Epoch: 720, Train Loss: 36.2667, Valid Loss: 54.9370\n",
      "Epoch: 721, Train Loss: 36.0132, Valid Loss: 55.4562\n",
      "Epoch: 722, Train Loss: 35.9572, Valid Loss: 57.3025\n",
      "Epoch: 723, Train Loss: 36.0747, Valid Loss: 55.1137\n",
      "Learning rate has been changed to: 1e-05\n",
      "Epoch: 724, Train Loss: 36.3128, Valid Loss: 53.5457\n",
      "Epoch: 725, Train Loss: 35.2586, Valid Loss: 53.6575\n",
      "Epoch: 726, Train Loss: 35.5524, Valid Loss: 55.8036\n",
      "Epoch: 727, Train Loss: 35.6398, Valid Loss: 54.1915\n",
      "Epoch: 728, Train Loss: 35.7354, Valid Loss: 53.1931\n",
      "Epoch: 729, Train Loss: 35.1659, Valid Loss: 54.6137\n",
      "Epoch: 730, Train Loss: 35.3486, Valid Loss: 56.6986\n",
      "Epoch: 731, Train Loss: 35.4226, Valid Loss: 55.9666\n",
      "Epoch: 732, Train Loss: 34.9428, Valid Loss: 56.7918\n",
      "Epoch: 733, Train Loss: 34.8166, Valid Loss: 55.4797\n",
      "Epoch: 734, Train Loss: 35.1739, Valid Loss: 56.1322\n",
      "Epoch: 735, Train Loss: 35.3354, Valid Loss: 55.1077\n",
      "Epoch: 736, Train Loss: 35.6273, Valid Loss: 56.1881\n",
      "Epoch: 737, Train Loss: 35.7273, Valid Loss: 54.4974\n",
      "Epoch: 738, Train Loss: 35.4188, Valid Loss: 56.4704\n",
      "Epoch: 739, Train Loss: 35.0502, Valid Loss: 55.4137\n",
      "Epoch: 740, Train Loss: 34.8997, Valid Loss: 53.7282\n",
      "Epoch: 741, Train Loss: 35.4234, Valid Loss: 54.8532\n",
      "Epoch: 742, Train Loss: 35.2096, Valid Loss: 53.3907\n",
      "Epoch: 743, Train Loss: 35.0594, Valid Loss: 56.3174\n",
      "Epoch: 744, Train Loss: 35.1725, Valid Loss: 55.2500\n",
      "Epoch: 745, Train Loss: 35.4153, Valid Loss: 56.1004\n",
      "Epoch: 746, Train Loss: 35.3788, Valid Loss: 56.4510\n",
      "Epoch: 747, Train Loss: 35.5313, Valid Loss: 56.7946\n",
      "Epoch: 748, Train Loss: 35.1442, Valid Loss: 55.5946\n",
      "Epoch: 749, Train Loss: 35.0406, Valid Loss: 57.5265\n",
      "Epoch: 750, Train Loss: 35.3830, Valid Loss: 56.6675\n",
      "Epoch: 751, Train Loss: 35.0725, Valid Loss: 55.4521\n",
      "Epoch: 752, Train Loss: 35.2188, Valid Loss: 54.1980\n",
      "Epoch: 753, Train Loss: 35.2261, Valid Loss: 55.9398\n",
      "Epoch: 754, Train Loss: 35.1144, Valid Loss: 55.5201\n",
      "Epoch: 755, Train Loss: 35.1979, Valid Loss: 57.0158\n",
      "Epoch: 756, Train Loss: 35.5084, Valid Loss: 55.7093\n",
      "Epoch: 757, Train Loss: 35.5707, Valid Loss: 55.6728\n",
      "Epoch: 758, Train Loss: 35.6351, Valid Loss: 56.2528\n",
      "Epoch: 759, Train Loss: 35.2434, Valid Loss: 56.0109\n",
      "Epoch: 760, Train Loss: 35.2418, Valid Loss: 54.2782\n",
      "Epoch: 761, Train Loss: 35.6268, Valid Loss: 57.1937\n",
      "Epoch: 762, Train Loss: 35.1982, Valid Loss: 55.7098\n",
      "Epoch: 763, Train Loss: 35.0592, Valid Loss: 56.7056\n",
      "Epoch: 764, Train Loss: 35.4269, Valid Loss: 53.0409\n",
      "Epoch: 765, Train Loss: 35.6147, Valid Loss: 56.8275\n",
      "Epoch: 766, Train Loss: 35.4810, Valid Loss: 55.0925\n",
      "Epoch: 767, Train Loss: 35.3979, Valid Loss: 56.1857\n",
      "Epoch: 768, Train Loss: 35.2265, Valid Loss: 56.1280\n",
      "Epoch: 769, Train Loss: 34.7750, Valid Loss: 56.7489\n",
      "Epoch: 770, Train Loss: 35.3805, Valid Loss: 53.6846\n",
      "Epoch: 771, Train Loss: 35.2764, Valid Loss: 52.7992\n",
      "Epoch: 772, Train Loss: 35.4930, Valid Loss: 56.3384\n",
      "Epoch: 773, Train Loss: 35.1810, Valid Loss: 54.1439\n",
      "Epoch: 774, Train Loss: 35.6133, Valid Loss: 55.6521\n",
      "Epoch: 775, Train Loss: 34.9538, Valid Loss: 55.2464\n",
      "Epoch: 776, Train Loss: 35.4721, Valid Loss: 56.1186\n",
      "Epoch: 777, Train Loss: 35.4543, Valid Loss: 54.6250\n",
      "Epoch: 778, Train Loss: 35.2110, Valid Loss: 55.3650\n",
      "Epoch: 779, Train Loss: 35.2400, Valid Loss: 54.7664\n",
      "Epoch: 780, Train Loss: 35.0285, Valid Loss: 55.1897\n",
      "Epoch: 781, Train Loss: 35.0592, Valid Loss: 53.1999\n",
      "Epoch: 782, Train Loss: 34.8410, Valid Loss: 56.1461\n",
      "Epoch: 783, Train Loss: 34.7822, Valid Loss: 56.3813\n",
      "Epoch: 784, Train Loss: 34.8787, Valid Loss: 55.0882\n",
      "Epoch: 785, Train Loss: 34.8481, Valid Loss: 55.7248\n",
      "Epoch: 786, Train Loss: 35.4510, Valid Loss: 57.3850\n",
      "Epoch: 787, Train Loss: 34.8618, Valid Loss: 55.0672\n",
      "Epoch: 788, Train Loss: 34.9081, Valid Loss: 53.6238\n",
      "Epoch: 789, Train Loss: 34.5903, Valid Loss: 56.0228\n",
      "Epoch: 790, Train Loss: 34.8218, Valid Loss: 54.8466\n",
      "Epoch: 791, Train Loss: 35.0555, Valid Loss: 56.2325\n",
      "Epoch: 792, Train Loss: 35.0757, Valid Loss: 55.5509\n",
      "Epoch: 793, Train Loss: 35.5903, Valid Loss: 52.0481\n",
      "Epoch: 794, Train Loss: 35.3333, Valid Loss: 54.7084\n",
      "Epoch: 795, Train Loss: 35.2194, Valid Loss: 54.4831\n",
      "Epoch: 796, Train Loss: 35.1486, Valid Loss: 56.6227\n",
      "Epoch: 797, Train Loss: 34.9713, Valid Loss: 55.8539\n",
      "Epoch: 798, Train Loss: 35.3280, Valid Loss: 55.9429\n",
      "Epoch: 799, Train Loss: 34.6591, Valid Loss: 55.9845\n",
      "Epoch: 800, Train Loss: 35.0222, Valid Loss: 56.6195\n",
      "Epoch: 801, Train Loss: 35.2621, Valid Loss: 55.7357\n",
      "Epoch: 802, Train Loss: 35.2731, Valid Loss: 56.4570\n",
      "Epoch: 803, Train Loss: 34.8091, Valid Loss: 54.4749\n",
      "Epoch: 804, Train Loss: 35.0919, Valid Loss: 59.2465\n",
      "Epoch: 805, Train Loss: 35.3449, Valid Loss: 56.3673\n",
      "Epoch: 806, Train Loss: 35.2628, Valid Loss: 53.5484\n",
      "Epoch: 807, Train Loss: 34.9811, Valid Loss: 54.5933\n",
      "Epoch: 808, Train Loss: 35.3252, Valid Loss: 55.7042\n",
      "Epoch: 809, Train Loss: 34.8910, Valid Loss: 56.1861\n",
      "Epoch: 810, Train Loss: 34.9932, Valid Loss: 54.5806\n",
      "Epoch: 811, Train Loss: 35.0319, Valid Loss: 54.6131\n",
      "Epoch: 812, Train Loss: 35.4857, Valid Loss: 54.1298\n",
      "Epoch: 813, Train Loss: 35.0850, Valid Loss: 54.7294\n",
      "Epoch: 814, Train Loss: 35.3622, Valid Loss: 55.4488\n",
      "Epoch: 815, Train Loss: 35.4391, Valid Loss: 54.1127\n",
      "Epoch: 816, Train Loss: 35.2462, Valid Loss: 54.6728\n",
      "Epoch: 817, Train Loss: 34.9678, Valid Loss: 53.3860\n",
      "Epoch: 818, Train Loss: 35.1945, Valid Loss: 57.6687\n",
      "Epoch: 819, Train Loss: 35.3278, Valid Loss: 55.2572\n",
      "Epoch: 820, Train Loss: 35.3811, Valid Loss: 57.1211\n",
      "Epoch: 821, Train Loss: 35.1698, Valid Loss: 56.7025\n",
      "Epoch: 822, Train Loss: 35.0835, Valid Loss: 53.9806\n",
      "Epoch: 823, Train Loss: 35.1204, Valid Loss: 56.4242\n",
      "Epoch: 824, Train Loss: 34.9396, Valid Loss: 55.4903\n",
      "Epoch: 825, Train Loss: 35.3804, Valid Loss: 52.6679\n",
      "Epoch: 826, Train Loss: 35.5919, Valid Loss: 55.7597\n",
      "Epoch: 827, Train Loss: 34.9516, Valid Loss: 55.1367\n",
      "Epoch: 828, Train Loss: 34.6477, Valid Loss: 57.0136\n",
      "Epoch: 829, Train Loss: 35.4076, Valid Loss: 57.0968\n",
      "Epoch: 830, Train Loss: 34.9728, Valid Loss: 54.0432\n",
      "Epoch: 831, Train Loss: 35.2028, Valid Loss: 55.6370\n",
      "Epoch: 832, Train Loss: 35.0029, Valid Loss: 54.2082\n",
      "Epoch: 833, Train Loss: 34.4759, Valid Loss: 54.1721\n",
      "Epoch: 834, Train Loss: 35.1964, Valid Loss: 55.0812\n",
      "Epoch: 835, Train Loss: 35.1959, Valid Loss: 52.8682\n",
      "Epoch: 836, Train Loss: 34.9069, Valid Loss: 56.1308\n",
      "Epoch: 837, Train Loss: 35.2068, Valid Loss: 55.3546\n",
      "Epoch: 838, Train Loss: 34.6920, Valid Loss: 55.7827\n",
      "Epoch: 839, Train Loss: 35.0484, Valid Loss: 54.9636\n",
      "Epoch: 840, Train Loss: 35.0521, Valid Loss: 54.6440\n",
      "Epoch: 841, Train Loss: 35.0042, Valid Loss: 54.4107\n",
      "Epoch: 842, Train Loss: 35.2628, Valid Loss: 53.7696\n",
      "Epoch: 843, Train Loss: 35.1204, Valid Loss: 54.2214\n",
      "Learning rate has been changed to: 1.0000000000000002e-06\n",
      "Epoch: 844, Train Loss: 34.7534, Valid Loss: 53.7290\n",
      "Epoch: 845, Train Loss: 34.4947, Valid Loss: 55.9013\n",
      "Epoch: 846, Train Loss: 34.8445, Valid Loss: 53.1207\n",
      "Epoch: 847, Train Loss: 34.9330, Valid Loss: 56.5820\n",
      "Epoch: 848, Train Loss: 35.0301, Valid Loss: 53.8533\n",
      "Epoch: 849, Train Loss: 35.0188, Valid Loss: 56.7823\n",
      "Epoch: 850, Train Loss: 34.9907, Valid Loss: 54.5919\n",
      "Epoch: 851, Train Loss: 35.6480, Valid Loss: 54.7619\n",
      "Epoch: 852, Train Loss: 34.8752, Valid Loss: 53.5856\n",
      "Epoch: 853, Train Loss: 34.7086, Valid Loss: 55.3559\n",
      "Epoch: 854, Train Loss: 34.5286, Valid Loss: 56.6504\n",
      "Epoch: 855, Train Loss: 34.8038, Valid Loss: 57.0606\n",
      "Epoch: 856, Train Loss: 34.7782, Valid Loss: 53.8772\n",
      "Epoch: 857, Train Loss: 35.0990, Valid Loss: 55.6435\n",
      "Epoch: 858, Train Loss: 35.1452, Valid Loss: 55.5318\n",
      "Epoch: 859, Train Loss: 35.0558, Valid Loss: 55.7160\n",
      "Epoch: 860, Train Loss: 35.1938, Valid Loss: 54.3783\n",
      "Epoch: 861, Train Loss: 34.8587, Valid Loss: 56.6840\n",
      "Epoch: 862, Train Loss: 34.8773, Valid Loss: 54.9554\n",
      "Epoch: 863, Train Loss: 35.0233, Valid Loss: 56.6742\n",
      "Epoch: 864, Train Loss: 35.0989, Valid Loss: 54.5587\n",
      "Epoch: 865, Train Loss: 34.8069, Valid Loss: 54.1197\n",
      "Epoch: 866, Train Loss: 34.8745, Valid Loss: 53.4894\n",
      "Epoch: 867, Train Loss: 34.8617, Valid Loss: 56.7544\n",
      "Epoch: 868, Train Loss: 35.2632, Valid Loss: 56.0250\n",
      "Epoch: 869, Train Loss: 36.0398, Valid Loss: 54.3542\n",
      "Epoch: 870, Train Loss: 35.0078, Valid Loss: 57.5172\n",
      "Epoch: 871, Train Loss: 35.3665, Valid Loss: 56.2176\n",
      "Epoch: 872, Train Loss: 35.1157, Valid Loss: 56.9471\n",
      "Epoch: 873, Train Loss: 34.9889, Valid Loss: 54.3785\n",
      "Epoch: 874, Train Loss: 34.4353, Valid Loss: 55.2779\n",
      "Epoch: 875, Train Loss: 34.7519, Valid Loss: 53.0173\n",
      "Epoch: 876, Train Loss: 35.0820, Valid Loss: 56.2958\n",
      "Epoch: 877, Train Loss: 35.0827, Valid Loss: 55.3089\n",
      "Epoch: 878, Train Loss: 35.1600, Valid Loss: 55.2953\n",
      "Epoch: 879, Train Loss: 35.0336, Valid Loss: 55.2590\n",
      "Epoch: 880, Train Loss: 34.7580, Valid Loss: 56.0449\n",
      "Epoch: 881, Train Loss: 34.6850, Valid Loss: 54.9462\n",
      "Epoch: 882, Train Loss: 35.3620, Valid Loss: 53.8908\n",
      "Epoch: 883, Train Loss: 35.1440, Valid Loss: 54.0526\n",
      "Epoch: 884, Train Loss: 34.8754, Valid Loss: 56.9017\n",
      "Epoch: 885, Train Loss: 34.8306, Valid Loss: 52.9969\n",
      "Epoch: 886, Train Loss: 34.9472, Valid Loss: 55.1327\n",
      "Epoch: 887, Train Loss: 34.6484, Valid Loss: 53.1946\n",
      "Epoch: 888, Train Loss: 35.0293, Valid Loss: 54.8594\n",
      "Epoch: 889, Train Loss: 35.1456, Valid Loss: 55.8782\n",
      "Epoch: 890, Train Loss: 35.0725, Valid Loss: 53.9984\n",
      "Epoch: 891, Train Loss: 34.9472, Valid Loss: 53.6157\n",
      "Epoch: 892, Train Loss: 34.3276, Valid Loss: 56.1309\n",
      "Epoch: 893, Train Loss: 35.5601, Valid Loss: 54.7359\n",
      "Epoch: 894, Train Loss: 35.2791, Valid Loss: 53.2377\n",
      "Learning rate has been changed to: 1.0000000000000002e-07\n",
      "Epoch: 895, Train Loss: 36.7045, Valid Loss: 54.0461\n"
     ]
    }
   ],
   "source": [
    "#直接训练模型，根据验证集MSE保存最优模型，超过50伦验证集MSE没有下降则修改学习率\n",
    "min_valid_loss = float('inf')\n",
    "no_improve = 0\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for features, targets in train_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model = model.cuda()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss+loss.item()\n",
    "    valid_loss = valid(model, valid_data, criterion)\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model, model_dir + str(min_valid_loss) + '.pth')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    if no_improve > 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "            print('Learning rate has been changed to: {}'.format(param_group['lr']))\n",
    "        no_improve = 0\n",
    "    print('Epoch: {}, Train Loss: {:.4f}, Valid Loss: {:.4f}'.format(epoch, train_loss / len(train_data), valid_loss))\n",
    "    #将打印的训练集MSE和验证集MSE保存到txt文件\n",
    "    with open(model_dir + 'loss.txt', 'a') as f:\n",
    "        f.write('Epoch: {}, Train Loss: {:.4f}, Valid Loss: {:.4f}\\n'.format(epoch, train_loss / len(train_data), valid_loss))\n",
    "    if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4447\n"
     ]
    }
   ],
   "source": [
    "model=torch.load(\"model/CNN_Transformer52.048096895217896.pth\")\n",
    "# model=torch.load(\"model/CNN_BiGRU_Attention238.75645760832163.pth\")\n",
    "#计算测试集MSE\n",
    "test_data_dir=\"data/test_data.xlsx\"\n",
    "test_data_loader = Test_Loader(test_data_dir, train_data_loader.scaler)\n",
    "test_data = DataLoader(test_data_loader, batch_size=1, shuffle=False)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features, targets in test_data:\n",
    "#     features = features.cuda()\n",
    "#     targets = targets.cuda()\n",
    "#     test_model = model.cuda()\n",
    "#     outputs = test_model(features)\n",
    "#     print(outputs.cpu().detach().numpy()[0][0][0])\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YS_RMSE: 9.1290,YS_MAPE: 0.0238,YS_R2: 0.9932\n",
      "TS_RMSE: 9.4785,TS_MAPE: 0.0148,TS_R2: 0.9962\n",
      "EL_RMSE: 1.8514,EL_MAPE: 0.0391,EL_R2: 0.9620\n"
     ]
    }
   ],
   "source": [
    "#计算测试集上的MSE,RMSE,MAE,R2\n",
    "def test(test_model, test_data):\n",
    "    test_model.eval()\n",
    "\n",
    "    YS_true = []\n",
    "    YS_pred = []\n",
    "\n",
    "    TS_true = []\n",
    "    TS_pred = []\n",
    "\n",
    "    EL_true = []\n",
    "    EL_pred = []\n",
    "\n",
    "    for features, targets in test_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        test_model = test_model.cuda()\n",
    "        outputs = test_model(features)\n",
    "        YS_true.append(targets.cpu().detach().numpy()[0][0][0])\n",
    "        YS_pred.append(outputs.cpu().detach().numpy()[0][0][0])\n",
    "        TS_true.append(targets.cpu().detach().numpy()[0][0][1])\n",
    "        TS_pred.append(outputs.cpu().detach().numpy()[0][0][1])\n",
    "        EL_true.append(targets.cpu().detach().numpy()[0][0][2])\n",
    "        EL_pred.append(outputs.cpu().detach().numpy()[0][0][2])\n",
    "    #计算YS的RMSE,MAPE,R2\n",
    "    YS_true = np.array(YS_true)\n",
    "    YS_pred = np.array(YS_pred)\n",
    "    YS_MSE = mean_squared_error(YS_true, YS_pred)\n",
    "    YS_RMSE = np.sqrt(YS_MSE)\n",
    "    YS_MAPE = np.mean(np.abs((YS_pred - YS_true) / YS_true))\n",
    "    YS_R2 = 1 - YS_MSE / np.var(YS_true)\n",
    "    print(f'YS_RMSE: {YS_RMSE:.4f},YS_MAPE: {YS_MAPE:.4f},YS_R2: {YS_R2:.4f}')\n",
    "    #计算TS的RMSE,MAPE,R2\n",
    "    TS_true = np.array(TS_true)\n",
    "    TS_pred = np.array(TS_pred)\n",
    "    TS_MSE = mean_squared_error(TS_true, TS_pred)\n",
    "    TS_RMSE = np.sqrt(TS_MSE)\n",
    "    TS_MAPE = np.mean(np.abs((TS_pred - TS_true) / TS_true))\n",
    "    TS_R2 = 1 - TS_MSE / np.var(TS_true)\n",
    "    print(f'TS_RMSE: {TS_RMSE:.4f},TS_MAPE: {TS_MAPE:.4f},TS_R2: {TS_R2:.4f}')\n",
    "    #计算EL的RMSE,MAPE,R2\n",
    "    EL_true = np.array(EL_true)\n",
    "    EL_pred = np.array(EL_pred)\n",
    "    EL_MSE = mean_squared_error(EL_true, EL_pred)\n",
    "    EL_RMSE = np.sqrt(EL_MSE)\n",
    "    EL_MAPE = np.mean(np.abs((EL_pred - EL_true) / EL_true))\n",
    "    EL_R2 = 1 - EL_MSE / np.var(EL_true)\n",
    "    print(f'EL_RMSE: {EL_RMSE:.4f},EL_MAPE: {EL_MAPE:.4f},EL_R2: {EL_R2:.4f}')\n",
    "    \n",
    "\n",
    "    return YS_pred, TS_pred, EL_pred, YS_true, TS_true, EL_true\n",
    "\n",
    "YS_pred, TS_pred, EL_pred, YS_true, TS_true, EL_true = test(model, test_data)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch121",
   "language": "python",
   "name": "pytorch121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
