{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成训练集\n",
    "class Train_Loader(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir, \n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        data = pd.read_excel(self.data_dir, skiprows=0)\n",
    "        #将数据类型转换为float\n",
    "        data = data.astype('float32')\n",
    "        #缺失值使用上一行和下一行同一列的平均值填充\n",
    "        data = data.fillna(data.mean())\n",
    "        #对目标列(除最后三列)归一化\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        data.iloc[:, :-3] = scaler.fit_transform(data.iloc[:, :-3])\n",
    "        self.data = data.values\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        features = self.data[i, :-3]\n",
    "        #扩充为二维\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        targets = self.data[i, -3:]\n",
    "        #扩充为二维\n",
    "        targets = np.expand_dims(targets, axis=0)\n",
    "        return features, targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "#生成测试集\n",
    "class Test_Loader(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir, \n",
    "            scaler,\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        data = pd.read_excel(self.data_dir, skiprows=0)\n",
    "        #将数据类型转换为float\n",
    "        data = data.astype('float32')\n",
    "        #缺失值使用上一行和下一行同一列的平均值填充\n",
    "        data = data.fillna(data.mean())\n",
    "        #对目标列(除最后三列)归一化\n",
    "        data.iloc[:, :-3] = scaler.transform(data.iloc[:, :-3])\n",
    "        self.data = data.values\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        features = self.data[i, :-3]\n",
    "        #扩充为二维\n",
    "        features = np.expand_dims(features, axis=0)\n",
    "        targets = self.data[i, -3:]\n",
    "        #扩充为二维\n",
    "        targets = np.expand_dims(targets, axis=0)\n",
    "        return features, targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir=\"data/train_data.xlsx\"\n",
    "train_data_loader = Train_Loader(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 65)\n",
      "(1, 3)\n",
      "20157\n"
     ]
    }
   ],
   "source": [
    "#查看feature，target数据形状\n",
    "features, targets = train_data_loader[0]\n",
    "print(features.shape)\n",
    "print(targets.shape)\n",
    "#查看train_data_loader数据长度\n",
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 65])\n",
      "torch.Size([64, 1, 3])\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "batchsize=64\n",
    "#创建数据加载器\n",
    "train_data = DataLoader(train_data_loader, batch_size=batchsize, shuffle=True)\n",
    "#查看数据加载器，数据形状\n",
    "for features, targets in train_data:\n",
    "    print(features.shape)\n",
    "    print(targets.shape)\n",
    "    break\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nets.CNN_BIGRU_Attention import CNN_BiGRU_Attention\n",
    "n_future = 65  # Example value, adjust as needed\n",
    "n_class = 3   # Example value, adjust as needed\n",
    "model = CNN_BiGRU_Attention(n_future, n_class)\n",
    "\n",
    "#定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "#定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model_dir=\"model/CNN_BiGRU_Attention\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=torch.load(\"model/CNN_BiGRU_Attention588.952.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "#计算验证集的MSE\n",
    "valid_data_dir=\"data/valid_data.xlsx\"\n",
    "valid_data_loader = Test_Loader(valid_data_dir, train_data_loader.scaler)\n",
    "valid_data = DataLoader(valid_data_loader, batch_size=batchsize, shuffle=False)\n",
    "print(len(valid_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(valid_model, valid_data, criterion):\n",
    "    # valid_model.eval()\n",
    "    valid_loss = 0\n",
    "    for features, targets in valid_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        valid_model = valid_model.cuda()\n",
    "        outputs = valid_model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        valid_loss = valid_loss+loss.item()\n",
    "    # print('valid_loss:', valid_loss / len(valid_data))\n",
    "    return valid_loss / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 50290.9064, Valid Loss: 13870.2512\n",
      "Epoch: 1, Train Loss: 12295.0644, Valid Loss: 12102.0718\n",
      "Epoch: 2, Train Loss: 5175.6430, Valid Loss: 719.7935\n",
      "Epoch: 3, Train Loss: 583.6627, Valid Loss: 497.3935\n",
      "Epoch: 4, Train Loss: 429.0909, Valid Loss: 388.9074\n",
      "Epoch: 5, Train Loss: 384.9320, Valid Loss: 370.0553\n",
      "Epoch: 6, Train Loss: 371.1830, Valid Loss: 362.8242\n",
      "Epoch: 7, Train Loss: 362.4310, Valid Loss: 349.6757\n",
      "Epoch: 8, Train Loss: 353.5115, Valid Loss: 337.1813\n",
      "Epoch: 9, Train Loss: 338.5733, Valid Loss: 319.3036\n",
      "Epoch: 10, Train Loss: 308.0131, Valid Loss: 273.6759\n",
      "Epoch: 11, Train Loss: 255.4690, Valid Loss: 220.2356\n",
      "Epoch: 12, Train Loss: 203.7031, Valid Loss: 179.2967\n",
      "Epoch: 13, Train Loss: 178.9815, Valid Loss: 164.1949\n",
      "Epoch: 14, Train Loss: 164.9173, Valid Loss: 157.4228\n",
      "Epoch: 15, Train Loss: 148.2069, Valid Loss: 139.7091\n",
      "Epoch: 16, Train Loss: 136.5460, Valid Loss: 139.0116\n",
      "Epoch: 17, Train Loss: 128.4460, Valid Loss: 129.5499\n",
      "Epoch: 18, Train Loss: 125.6399, Valid Loss: 128.1354\n",
      "Epoch: 19, Train Loss: 122.7643, Valid Loss: 125.7930\n",
      "Epoch: 20, Train Loss: 121.0797, Valid Loss: 124.8691\n",
      "Epoch: 21, Train Loss: 119.9725, Valid Loss: 122.6761\n",
      "Epoch: 22, Train Loss: 120.9282, Valid Loss: 129.1557\n",
      "Epoch: 23, Train Loss: 116.2057, Valid Loss: 120.2988\n",
      "Epoch: 24, Train Loss: 116.5061, Valid Loss: 123.3766\n",
      "Epoch: 25, Train Loss: 115.6958, Valid Loss: 121.1958\n",
      "Epoch: 26, Train Loss: 114.8755, Valid Loss: 116.0848\n",
      "Epoch: 27, Train Loss: 112.0959, Valid Loss: 116.4853\n",
      "Epoch: 28, Train Loss: 111.1877, Valid Loss: 117.9313\n",
      "Epoch: 29, Train Loss: 110.8539, Valid Loss: 112.1342\n",
      "Epoch: 30, Train Loss: 109.7291, Valid Loss: 113.4483\n",
      "Epoch: 31, Train Loss: 109.3928, Valid Loss: 111.3507\n",
      "Epoch: 32, Train Loss: 109.7650, Valid Loss: 114.4493\n",
      "Epoch: 33, Train Loss: 107.6584, Valid Loss: 110.1721\n",
      "Epoch: 34, Train Loss: 107.9652, Valid Loss: 110.6146\n",
      "Epoch: 35, Train Loss: 106.2389, Valid Loss: 110.0431\n",
      "Epoch: 36, Train Loss: 104.8728, Valid Loss: 114.0047\n",
      "Epoch: 37, Train Loss: 106.6360, Valid Loss: 110.9203\n",
      "Epoch: 38, Train Loss: 103.3165, Valid Loss: 105.6345\n",
      "Epoch: 39, Train Loss: 102.6216, Valid Loss: 108.0004\n",
      "Epoch: 40, Train Loss: 101.3088, Valid Loss: 110.9505\n",
      "Epoch: 41, Train Loss: 101.2923, Valid Loss: 103.6571\n",
      "Epoch: 42, Train Loss: 98.6767, Valid Loss: 106.8880\n",
      "Epoch: 43, Train Loss: 97.9524, Valid Loss: 100.6927\n",
      "Epoch: 44, Train Loss: 98.3644, Valid Loss: 107.0050\n",
      "Epoch: 45, Train Loss: 97.0003, Valid Loss: 102.2322\n",
      "Epoch: 46, Train Loss: 94.8924, Valid Loss: 100.4115\n",
      "Epoch: 47, Train Loss: 94.0604, Valid Loss: 101.8367\n",
      "Epoch: 48, Train Loss: 93.2419, Valid Loss: 98.2885\n",
      "Epoch: 49, Train Loss: 93.3485, Valid Loss: 99.8094\n",
      "Epoch: 50, Train Loss: 91.1023, Valid Loss: 98.8206\n",
      "Epoch: 51, Train Loss: 90.8241, Valid Loss: 104.5352\n",
      "Epoch: 52, Train Loss: 92.1342, Valid Loss: 113.7419\n",
      "Epoch: 53, Train Loss: 89.9607, Valid Loss: 96.4938\n",
      "Epoch: 54, Train Loss: 90.6382, Valid Loss: 98.1986\n",
      "Epoch: 55, Train Loss: 89.5638, Valid Loss: 93.5326\n",
      "Epoch: 56, Train Loss: 89.8588, Valid Loss: 131.5529\n",
      "Epoch: 57, Train Loss: 90.3733, Valid Loss: 102.1964\n",
      "Epoch: 58, Train Loss: 87.2343, Valid Loss: 92.3916\n",
      "Epoch: 59, Train Loss: 87.0999, Valid Loss: 93.6116\n",
      "Epoch: 60, Train Loss: 87.8632, Valid Loss: 91.6078\n",
      "Epoch: 61, Train Loss: 86.8339, Valid Loss: 97.8964\n",
      "Epoch: 62, Train Loss: 86.8662, Valid Loss: 106.7204\n",
      "Epoch: 63, Train Loss: 85.8299, Valid Loss: 91.0294\n",
      "Epoch: 64, Train Loss: 85.7286, Valid Loss: 91.0451\n",
      "Epoch: 65, Train Loss: 84.7143, Valid Loss: 90.7649\n",
      "Epoch: 66, Train Loss: 84.2966, Valid Loss: 88.5405\n",
      "Epoch: 67, Train Loss: 83.6657, Valid Loss: 89.1325\n",
      "Epoch: 68, Train Loss: 83.8853, Valid Loss: 92.3341\n",
      "Epoch: 69, Train Loss: 84.5994, Valid Loss: 91.9836\n",
      "Epoch: 70, Train Loss: 83.5661, Valid Loss: 91.3046\n",
      "Epoch: 71, Train Loss: 83.4676, Valid Loss: 96.8725\n",
      "Epoch: 72, Train Loss: 82.9843, Valid Loss: 85.9230\n",
      "Epoch: 73, Train Loss: 82.9297, Valid Loss: 91.6169\n",
      "Epoch: 74, Train Loss: 83.4075, Valid Loss: 90.3244\n",
      "Epoch: 75, Train Loss: 82.8322, Valid Loss: 87.1459\n",
      "Epoch: 76, Train Loss: 81.7230, Valid Loss: 90.2558\n",
      "Epoch: 77, Train Loss: 80.0171, Valid Loss: 86.9962\n",
      "Epoch: 78, Train Loss: 80.6788, Valid Loss: 89.0989\n",
      "Epoch: 79, Train Loss: 80.6581, Valid Loss: 85.8441\n",
      "Epoch: 80, Train Loss: 81.9754, Valid Loss: 88.7719\n",
      "Epoch: 81, Train Loss: 78.9446, Valid Loss: 84.8796\n",
      "Epoch: 82, Train Loss: 79.3602, Valid Loss: 84.1063\n",
      "Epoch: 83, Train Loss: 78.3428, Valid Loss: 88.0999\n",
      "Epoch: 84, Train Loss: 78.3066, Valid Loss: 85.9235\n",
      "Epoch: 85, Train Loss: 80.8315, Valid Loss: 85.5271\n",
      "Epoch: 86, Train Loss: 78.9746, Valid Loss: 85.0785\n",
      "Epoch: 87, Train Loss: 78.8475, Valid Loss: 86.9029\n",
      "Epoch: 88, Train Loss: 78.6621, Valid Loss: 85.2779\n",
      "Epoch: 89, Train Loss: 77.0422, Valid Loss: 83.6233\n",
      "Epoch: 90, Train Loss: 77.4887, Valid Loss: 88.9576\n",
      "Epoch: 91, Train Loss: 77.8840, Valid Loss: 85.4140\n",
      "Epoch: 92, Train Loss: 76.2934, Valid Loss: 85.5578\n",
      "Epoch: 93, Train Loss: 76.1735, Valid Loss: 83.3696\n",
      "Epoch: 94, Train Loss: 79.0034, Valid Loss: 86.4576\n",
      "Epoch: 95, Train Loss: 76.7102, Valid Loss: 84.2327\n",
      "Epoch: 96, Train Loss: 76.8975, Valid Loss: 83.6419\n",
      "Epoch: 97, Train Loss: 75.2405, Valid Loss: 81.5693\n",
      "Epoch: 98, Train Loss: 75.5668, Valid Loss: 81.1161\n",
      "Epoch: 99, Train Loss: 74.7701, Valid Loss: 83.9295\n",
      "Epoch: 100, Train Loss: 76.1991, Valid Loss: 80.3616\n",
      "Epoch: 101, Train Loss: 74.1311, Valid Loss: 82.4329\n",
      "Epoch: 102, Train Loss: 74.6230, Valid Loss: 84.0879\n",
      "Epoch: 103, Train Loss: 74.8968, Valid Loss: 80.6835\n",
      "Epoch: 104, Train Loss: 73.0058, Valid Loss: 86.9912\n",
      "Epoch: 105, Train Loss: 72.8975, Valid Loss: 93.7470\n",
      "Epoch: 106, Train Loss: 77.9004, Valid Loss: 90.1108\n",
      "Epoch: 107, Train Loss: 73.9869, Valid Loss: 79.3868\n",
      "Epoch: 108, Train Loss: 72.5664, Valid Loss: 80.7846\n",
      "Epoch: 109, Train Loss: 72.0086, Valid Loss: 80.1582\n",
      "Epoch: 110, Train Loss: 72.0732, Valid Loss: 80.8744\n",
      "Epoch: 111, Train Loss: 71.4575, Valid Loss: 84.8237\n",
      "Epoch: 112, Train Loss: 71.7480, Valid Loss: 80.9389\n",
      "Epoch: 113, Train Loss: 71.9105, Valid Loss: 77.5974\n",
      "Epoch: 114, Train Loss: 72.1430, Valid Loss: 81.0334\n",
      "Epoch: 115, Train Loss: 71.4442, Valid Loss: 80.2987\n",
      "Epoch: 116, Train Loss: 71.1485, Valid Loss: 78.7494\n",
      "Epoch: 117, Train Loss: 69.9243, Valid Loss: 80.7051\n",
      "Epoch: 118, Train Loss: 71.6091, Valid Loss: 77.9328\n",
      "Epoch: 119, Train Loss: 70.7547, Valid Loss: 79.0709\n",
      "Epoch: 120, Train Loss: 69.6534, Valid Loss: 78.3481\n",
      "Epoch: 121, Train Loss: 69.8988, Valid Loss: 84.2839\n",
      "Epoch: 122, Train Loss: 70.2809, Valid Loss: 79.8390\n",
      "Epoch: 123, Train Loss: 69.1728, Valid Loss: 81.2571\n",
      "Epoch: 124, Train Loss: 69.6237, Valid Loss: 79.6570\n",
      "Epoch: 125, Train Loss: 70.0085, Valid Loss: 80.5413\n",
      "Epoch: 126, Train Loss: 68.7656, Valid Loss: 79.2140\n",
      "Epoch: 127, Train Loss: 68.4666, Valid Loss: 78.4576\n",
      "Epoch: 128, Train Loss: 69.3835, Valid Loss: 78.2966\n",
      "Epoch: 129, Train Loss: 70.2519, Valid Loss: 77.2508\n",
      "Epoch: 130, Train Loss: 68.2067, Valid Loss: 82.9676\n",
      "Epoch: 131, Train Loss: 69.2116, Valid Loss: 81.2267\n",
      "Epoch: 132, Train Loss: 69.4745, Valid Loss: 78.3204\n",
      "Epoch: 133, Train Loss: 68.2965, Valid Loss: 80.3831\n",
      "Epoch: 134, Train Loss: 68.2018, Valid Loss: 80.8052\n",
      "Epoch: 135, Train Loss: 66.8027, Valid Loss: 79.3227\n",
      "Epoch: 136, Train Loss: 66.8352, Valid Loss: 77.2297\n",
      "Epoch: 137, Train Loss: 67.2298, Valid Loss: 75.0573\n",
      "Epoch: 138, Train Loss: 66.3596, Valid Loss: 83.6607\n",
      "Epoch: 139, Train Loss: 69.1777, Valid Loss: 76.7082\n",
      "Epoch: 140, Train Loss: 66.8282, Valid Loss: 77.7996\n",
      "Epoch: 141, Train Loss: 66.0298, Valid Loss: 80.0388\n",
      "Epoch: 142, Train Loss: 66.3222, Valid Loss: 80.8110\n",
      "Epoch: 143, Train Loss: 65.8646, Valid Loss: 77.8460\n",
      "Epoch: 144, Train Loss: 66.2370, Valid Loss: 75.2113\n",
      "Epoch: 145, Train Loss: 66.0324, Valid Loss: 82.5599\n",
      "Epoch: 146, Train Loss: 65.5519, Valid Loss: 78.1238\n",
      "Epoch: 147, Train Loss: 64.7608, Valid Loss: 81.8451\n",
      "Epoch: 148, Train Loss: 67.0570, Valid Loss: 82.3838\n",
      "Epoch: 149, Train Loss: 70.3968, Valid Loss: 76.1838\n",
      "Epoch: 150, Train Loss: 65.2253, Valid Loss: 76.7765\n",
      "Epoch: 151, Train Loss: 64.1647, Valid Loss: 82.2367\n",
      "Epoch: 152, Train Loss: 64.8973, Valid Loss: 78.0902\n",
      "Epoch: 153, Train Loss: 64.7432, Valid Loss: 76.9356\n",
      "Epoch: 154, Train Loss: 65.1942, Valid Loss: 75.8327\n",
      "Epoch: 155, Train Loss: 63.9239, Valid Loss: 75.5040\n",
      "Epoch: 156, Train Loss: 64.1596, Valid Loss: 76.6603\n",
      "Epoch: 157, Train Loss: 63.6090, Valid Loss: 78.4418\n",
      "Epoch: 158, Train Loss: 64.2121, Valid Loss: 77.4833\n",
      "Epoch: 159, Train Loss: 62.7706, Valid Loss: 77.1333\n",
      "Epoch: 160, Train Loss: 63.7124, Valid Loss: 77.9975\n",
      "Epoch: 161, Train Loss: 63.3432, Valid Loss: 75.7404\n",
      "Epoch: 162, Train Loss: 62.3058, Valid Loss: 73.9733\n",
      "Epoch: 163, Train Loss: 63.2371, Valid Loss: 78.8708\n",
      "Epoch: 164, Train Loss: 62.0649, Valid Loss: 74.6063\n",
      "Epoch: 165, Train Loss: 64.2707, Valid Loss: 74.2161\n",
      "Epoch: 166, Train Loss: 61.4927, Valid Loss: 81.9591\n",
      "Epoch: 167, Train Loss: 62.6079, Valid Loss: 75.4055\n",
      "Epoch: 168, Train Loss: 63.3271, Valid Loss: 72.6397\n",
      "Epoch: 169, Train Loss: 61.7070, Valid Loss: 74.6454\n",
      "Epoch: 170, Train Loss: 62.9719, Valid Loss: 73.4267\n",
      "Epoch: 171, Train Loss: 61.8143, Valid Loss: 73.9892\n",
      "Epoch: 172, Train Loss: 62.0456, Valid Loss: 80.5025\n",
      "Epoch: 173, Train Loss: 62.0734, Valid Loss: 78.1124\n",
      "Epoch: 174, Train Loss: 62.5930, Valid Loss: 79.1587\n",
      "Epoch: 175, Train Loss: 62.5940, Valid Loss: 81.3356\n",
      "Epoch: 176, Train Loss: 61.5150, Valid Loss: 75.4101\n",
      "Epoch: 177, Train Loss: 62.1271, Valid Loss: 72.5886\n",
      "Epoch: 178, Train Loss: 60.7963, Valid Loss: 73.1295\n",
      "Epoch: 179, Train Loss: 61.6727, Valid Loss: 75.8992\n",
      "Epoch: 180, Train Loss: 63.8439, Valid Loss: 79.7955\n",
      "Epoch: 181, Train Loss: 60.6398, Valid Loss: 76.5508\n",
      "Epoch: 182, Train Loss: 59.7940, Valid Loss: 72.5867\n",
      "Epoch: 183, Train Loss: 60.3856, Valid Loss: 74.9964\n",
      "Epoch: 184, Train Loss: 58.4233, Valid Loss: 72.2071\n",
      "Epoch: 185, Train Loss: 59.6761, Valid Loss: 72.9960\n",
      "Epoch: 186, Train Loss: 60.5001, Valid Loss: 76.4387\n",
      "Epoch: 187, Train Loss: 59.8360, Valid Loss: 75.2087\n",
      "Epoch: 188, Train Loss: 61.1896, Valid Loss: 78.7374\n",
      "Epoch: 189, Train Loss: 58.8596, Valid Loss: 73.5190\n",
      "Epoch: 190, Train Loss: 59.5162, Valid Loss: 75.1776\n",
      "Epoch: 191, Train Loss: 59.3525, Valid Loss: 73.1571\n",
      "Epoch: 192, Train Loss: 59.2374, Valid Loss: 85.5686\n",
      "Epoch: 193, Train Loss: 60.0683, Valid Loss: 74.3047\n",
      "Epoch: 194, Train Loss: 57.5810, Valid Loss: 73.0781\n",
      "Epoch: 195, Train Loss: 57.8894, Valid Loss: 73.4123\n",
      "Epoch: 196, Train Loss: 58.8292, Valid Loss: 71.0443\n",
      "Epoch: 197, Train Loss: 59.3050, Valid Loss: 71.1142\n",
      "Epoch: 198, Train Loss: 56.8511, Valid Loss: 69.3066\n",
      "Epoch: 199, Train Loss: 59.3394, Valid Loss: 83.5859\n",
      "Epoch: 200, Train Loss: 57.3513, Valid Loss: 71.5145\n",
      "Epoch: 201, Train Loss: 57.4219, Valid Loss: 72.3065\n",
      "Epoch: 202, Train Loss: 57.3420, Valid Loss: 83.9791\n",
      "Epoch: 203, Train Loss: 58.0503, Valid Loss: 81.7039\n",
      "Epoch: 204, Train Loss: 58.2291, Valid Loss: 72.3645\n",
      "Epoch: 205, Train Loss: 57.4757, Valid Loss: 72.6736\n",
      "Epoch: 206, Train Loss: 56.1847, Valid Loss: 69.7387\n",
      "Epoch: 207, Train Loss: 59.4027, Valid Loss: 80.7403\n",
      "Epoch: 208, Train Loss: 57.3986, Valid Loss: 70.4728\n",
      "Epoch: 209, Train Loss: 57.1019, Valid Loss: 71.2423\n",
      "Epoch: 210, Train Loss: 60.1579, Valid Loss: 73.8794\n",
      "Epoch: 211, Train Loss: 56.6912, Valid Loss: 69.7916\n",
      "Epoch: 212, Train Loss: 56.1126, Valid Loss: 70.0563\n",
      "Epoch: 213, Train Loss: 55.3142, Valid Loss: 74.7240\n",
      "Epoch: 214, Train Loss: 55.9635, Valid Loss: 68.6774\n",
      "Epoch: 215, Train Loss: 56.1479, Valid Loss: 73.8409\n",
      "Epoch: 216, Train Loss: 54.7129, Valid Loss: 71.1410\n",
      "Epoch: 217, Train Loss: 56.5772, Valid Loss: 72.0930\n",
      "Epoch: 218, Train Loss: 55.0686, Valid Loss: 77.9301\n",
      "Epoch: 219, Train Loss: 55.0712, Valid Loss: 70.0315\n",
      "Epoch: 220, Train Loss: 54.8276, Valid Loss: 74.3593\n",
      "Epoch: 221, Train Loss: 56.5048, Valid Loss: 89.7470\n",
      "Epoch: 222, Train Loss: 56.1894, Valid Loss: 70.6254\n",
      "Epoch: 223, Train Loss: 55.7133, Valid Loss: 73.3747\n",
      "Epoch: 224, Train Loss: 55.4274, Valid Loss: 69.8878\n",
      "Epoch: 225, Train Loss: 54.5680, Valid Loss: 78.4020\n",
      "Epoch: 226, Train Loss: 56.5884, Valid Loss: 70.0300\n",
      "Epoch: 227, Train Loss: 55.8925, Valid Loss: 76.4470\n",
      "Epoch: 228, Train Loss: 56.0778, Valid Loss: 69.8783\n",
      "Epoch: 229, Train Loss: 54.3652, Valid Loss: 68.9234\n",
      "Epoch: 230, Train Loss: 53.4988, Valid Loss: 72.5806\n",
      "Epoch: 231, Train Loss: 54.4208, Valid Loss: 69.5661\n",
      "Epoch: 232, Train Loss: 53.8131, Valid Loss: 72.7358\n",
      "Epoch: 233, Train Loss: 54.9460, Valid Loss: 66.8993\n",
      "Epoch: 234, Train Loss: 54.1669, Valid Loss: 72.7575\n",
      "Epoch: 235, Train Loss: 54.2273, Valid Loss: 68.4565\n",
      "Epoch: 236, Train Loss: 53.0254, Valid Loss: 72.3984\n",
      "Epoch: 237, Train Loss: 53.4412, Valid Loss: 69.3683\n",
      "Epoch: 238, Train Loss: 53.2224, Valid Loss: 69.3664\n",
      "Epoch: 239, Train Loss: 53.1031, Valid Loss: 68.1433\n",
      "Epoch: 240, Train Loss: 54.4414, Valid Loss: 69.7057\n",
      "Epoch: 241, Train Loss: 52.8248, Valid Loss: 68.1570\n",
      "Epoch: 242, Train Loss: 54.2766, Valid Loss: 74.1525\n",
      "Epoch: 243, Train Loss: 54.1185, Valid Loss: 70.4721\n",
      "Epoch: 244, Train Loss: 51.5057, Valid Loss: 69.2528\n",
      "Epoch: 245, Train Loss: 52.8024, Valid Loss: 69.2853\n",
      "Epoch: 246, Train Loss: 51.9807, Valid Loss: 70.4244\n",
      "Epoch: 247, Train Loss: 52.2844, Valid Loss: 68.5691\n",
      "Epoch: 248, Train Loss: 52.6146, Valid Loss: 72.8723\n",
      "Epoch: 249, Train Loss: 51.5608, Valid Loss: 67.5027\n",
      "Epoch: 250, Train Loss: 53.2303, Valid Loss: 73.0173\n",
      "Epoch: 251, Train Loss: 51.4798, Valid Loss: 68.3276\n",
      "Epoch: 252, Train Loss: 52.1519, Valid Loss: 68.2819\n",
      "Epoch: 253, Train Loss: 51.3050, Valid Loss: 66.2193\n",
      "Epoch: 254, Train Loss: 52.6311, Valid Loss: 72.0526\n",
      "Epoch: 255, Train Loss: 56.5200, Valid Loss: 68.6691\n",
      "Epoch: 256, Train Loss: 51.0027, Valid Loss: 67.6047\n",
      "Epoch: 257, Train Loss: 51.0196, Valid Loss: 68.2431\n",
      "Epoch: 258, Train Loss: 50.2810, Valid Loss: 67.4150\n",
      "Epoch: 259, Train Loss: 51.1834, Valid Loss: 68.9464\n",
      "Epoch: 260, Train Loss: 53.3419, Valid Loss: 68.5452\n",
      "Epoch: 261, Train Loss: 51.9418, Valid Loss: 71.7357\n",
      "Epoch: 262, Train Loss: 50.5082, Valid Loss: 71.8197\n",
      "Epoch: 263, Train Loss: 50.5843, Valid Loss: 69.6145\n",
      "Epoch: 264, Train Loss: 50.6058, Valid Loss: 64.2917\n",
      "Epoch: 265, Train Loss: 49.4719, Valid Loss: 71.8358\n",
      "Epoch: 266, Train Loss: 51.5582, Valid Loss: 66.5681\n",
      "Epoch: 267, Train Loss: 49.9897, Valid Loss: 69.2183\n",
      "Epoch: 268, Train Loss: 52.5300, Valid Loss: 71.8351\n",
      "Epoch: 269, Train Loss: 50.0708, Valid Loss: 66.2137\n",
      "Epoch: 270, Train Loss: 50.2778, Valid Loss: 67.2148\n",
      "Epoch: 271, Train Loss: 50.3844, Valid Loss: 71.9537\n",
      "Epoch: 272, Train Loss: 50.6734, Valid Loss: 65.3424\n",
      "Epoch: 273, Train Loss: 49.3512, Valid Loss: 64.9201\n",
      "Epoch: 274, Train Loss: 49.5997, Valid Loss: 68.1377\n",
      "Epoch: 275, Train Loss: 51.6402, Valid Loss: 74.0784\n",
      "Epoch: 276, Train Loss: 49.8096, Valid Loss: 64.9015\n",
      "Epoch: 277, Train Loss: 48.5415, Valid Loss: 68.0898\n",
      "Epoch: 278, Train Loss: 49.2713, Valid Loss: 73.0907\n",
      "Epoch: 279, Train Loss: 49.3418, Valid Loss: 65.2814\n",
      "Epoch: 280, Train Loss: 51.7107, Valid Loss: 68.8843\n",
      "Epoch: 281, Train Loss: 50.9107, Valid Loss: 65.9793\n",
      "Epoch: 282, Train Loss: 48.8489, Valid Loss: 67.3594\n",
      "Epoch: 283, Train Loss: 48.8376, Valid Loss: 65.9404\n",
      "Epoch: 284, Train Loss: 49.0260, Valid Loss: 67.4824\n",
      "Epoch: 285, Train Loss: 48.7749, Valid Loss: 65.2119\n",
      "Epoch: 286, Train Loss: 54.7589, Valid Loss: 67.8023\n",
      "Epoch: 287, Train Loss: 48.7602, Valid Loss: 62.7452\n",
      "Epoch: 288, Train Loss: 48.7246, Valid Loss: 66.2754\n",
      "Epoch: 289, Train Loss: 49.1540, Valid Loss: 69.1474\n",
      "Epoch: 290, Train Loss: 47.8417, Valid Loss: 63.6950\n",
      "Epoch: 291, Train Loss: 47.8986, Valid Loss: 68.1243\n",
      "Epoch: 292, Train Loss: 50.3660, Valid Loss: 70.3582\n",
      "Epoch: 293, Train Loss: 48.1374, Valid Loss: 66.7716\n",
      "Epoch: 294, Train Loss: 48.6389, Valid Loss: 65.4360\n",
      "Epoch: 295, Train Loss: 49.1097, Valid Loss: 75.1165\n",
      "Epoch: 296, Train Loss: 48.7374, Valid Loss: 63.1506\n",
      "Epoch: 297, Train Loss: 46.9778, Valid Loss: 67.5663\n",
      "Epoch: 298, Train Loss: 47.9946, Valid Loss: 65.2657\n",
      "Epoch: 299, Train Loss: 49.1513, Valid Loss: 63.7893\n",
      "Epoch: 300, Train Loss: 47.9736, Valid Loss: 67.4464\n",
      "Epoch: 301, Train Loss: 47.7762, Valid Loss: 64.4574\n",
      "Epoch: 302, Train Loss: 59.3886, Valid Loss: 63.9084\n",
      "Epoch: 303, Train Loss: 47.6565, Valid Loss: 67.0584\n",
      "Epoch: 304, Train Loss: 46.1827, Valid Loss: 65.5405\n",
      "Epoch: 305, Train Loss: 46.9660, Valid Loss: 63.5073\n",
      "Epoch: 306, Train Loss: 47.0447, Valid Loss: 63.9295\n",
      "Epoch: 307, Train Loss: 47.0821, Valid Loss: 65.8843\n",
      "Epoch: 308, Train Loss: 47.7142, Valid Loss: 68.0842\n",
      "Epoch: 309, Train Loss: 46.7258, Valid Loss: 69.6456\n",
      "Epoch: 310, Train Loss: 47.6983, Valid Loss: 67.6874\n",
      "Epoch: 311, Train Loss: 46.5351, Valid Loss: 66.6678\n",
      "Epoch: 312, Train Loss: 47.0437, Valid Loss: 62.4010\n",
      "Epoch: 313, Train Loss: 47.0869, Valid Loss: 64.2437\n",
      "Epoch: 314, Train Loss: 47.2058, Valid Loss: 64.1421\n",
      "Epoch: 315, Train Loss: 47.7240, Valid Loss: 63.1121\n",
      "Epoch: 316, Train Loss: 45.7951, Valid Loss: 65.0266\n",
      "Epoch: 317, Train Loss: 46.4585, Valid Loss: 61.6369\n",
      "Epoch: 318, Train Loss: 46.6015, Valid Loss: 68.0137\n",
      "Epoch: 319, Train Loss: 46.5503, Valid Loss: 64.5140\n",
      "Epoch: 320, Train Loss: 46.0211, Valid Loss: 63.4118\n",
      "Epoch: 321, Train Loss: 47.8117, Valid Loss: 64.7383\n",
      "Epoch: 322, Train Loss: 51.1803, Valid Loss: 61.6751\n",
      "Epoch: 323, Train Loss: 46.5908, Valid Loss: 68.7307\n",
      "Epoch: 324, Train Loss: 46.0072, Valid Loss: 66.7640\n",
      "Epoch: 325, Train Loss: 45.8976, Valid Loss: 63.5783\n",
      "Epoch: 326, Train Loss: 45.4232, Valid Loss: 62.8006\n",
      "Epoch: 327, Train Loss: 46.3776, Valid Loss: 72.8354\n",
      "Epoch: 328, Train Loss: 45.2679, Valid Loss: 60.8897\n",
      "Epoch: 329, Train Loss: 45.7289, Valid Loss: 68.0186\n",
      "Epoch: 330, Train Loss: 46.3817, Valid Loss: 63.8075\n",
      "Epoch: 331, Train Loss: 45.8195, Valid Loss: 61.8578\n",
      "Epoch: 332, Train Loss: 46.4515, Valid Loss: 65.2717\n",
      "Epoch: 333, Train Loss: 46.6351, Valid Loss: 63.3895\n",
      "Epoch: 334, Train Loss: 47.1395, Valid Loss: 61.4425\n",
      "Epoch: 335, Train Loss: 46.1058, Valid Loss: 62.1822\n",
      "Epoch: 336, Train Loss: 45.4666, Valid Loss: 62.2932\n",
      "Epoch: 337, Train Loss: 45.4406, Valid Loss: 65.3790\n",
      "Epoch: 338, Train Loss: 45.1795, Valid Loss: 62.7938\n",
      "Epoch: 339, Train Loss: 45.3884, Valid Loss: 60.9141\n",
      "Epoch: 340, Train Loss: 45.6200, Valid Loss: 66.4483\n",
      "Epoch: 341, Train Loss: 46.0459, Valid Loss: 67.3534\n",
      "Epoch: 342, Train Loss: 49.1413, Valid Loss: 75.3075\n",
      "Epoch: 343, Train Loss: 48.5560, Valid Loss: 61.0796\n",
      "Epoch: 344, Train Loss: 44.9908, Valid Loss: 101.3951\n",
      "Epoch: 345, Train Loss: 47.6989, Valid Loss: 69.2842\n",
      "Epoch: 346, Train Loss: 44.7376, Valid Loss: 60.7930\n",
      "Epoch: 347, Train Loss: 43.7797, Valid Loss: 59.9017\n",
      "Epoch: 348, Train Loss: 45.0812, Valid Loss: 62.0695\n",
      "Epoch: 349, Train Loss: 44.2165, Valid Loss: 66.2141\n",
      "Epoch: 350, Train Loss: 44.1018, Valid Loss: 67.4767\n",
      "Epoch: 351, Train Loss: 44.2999, Valid Loss: 67.8382\n",
      "Epoch: 352, Train Loss: 44.6693, Valid Loss: 65.1658\n",
      "Epoch: 353, Train Loss: 45.3792, Valid Loss: 69.4801\n",
      "Epoch: 354, Train Loss: 45.3086, Valid Loss: 61.8748\n",
      "Epoch: 355, Train Loss: 43.1334, Valid Loss: 63.6102\n",
      "Epoch: 356, Train Loss: 44.6491, Valid Loss: 61.4205\n",
      "Epoch: 357, Train Loss: 44.9317, Valid Loss: 61.9851\n",
      "Epoch: 358, Train Loss: 43.0402, Valid Loss: 62.0858\n",
      "Epoch: 359, Train Loss: 44.9086, Valid Loss: 61.0449\n",
      "Epoch: 360, Train Loss: 44.6763, Valid Loss: 66.4903\n",
      "Epoch: 361, Train Loss: 44.1239, Valid Loss: 66.8586\n",
      "Epoch: 362, Train Loss: 43.8258, Valid Loss: 59.7952\n",
      "Epoch: 363, Train Loss: 43.4204, Valid Loss: 60.0604\n",
      "Epoch: 364, Train Loss: 43.6870, Valid Loss: 63.8111\n",
      "Epoch: 365, Train Loss: 43.5555, Valid Loss: 64.8736\n",
      "Epoch: 366, Train Loss: 43.4492, Valid Loss: 59.3376\n",
      "Epoch: 367, Train Loss: 43.9752, Valid Loss: 62.0888\n",
      "Epoch: 368, Train Loss: 44.0387, Valid Loss: 67.3401\n",
      "Epoch: 369, Train Loss: 44.0882, Valid Loss: 63.0274\n",
      "Epoch: 370, Train Loss: 42.6210, Valid Loss: 63.3292\n",
      "Epoch: 371, Train Loss: 44.0020, Valid Loss: 60.4853\n",
      "Epoch: 372, Train Loss: 43.5313, Valid Loss: 62.2136\n",
      "Epoch: 373, Train Loss: 43.2660, Valid Loss: 61.2936\n",
      "Epoch: 374, Train Loss: 43.5681, Valid Loss: 69.7845\n",
      "Epoch: 375, Train Loss: 44.1828, Valid Loss: 61.5211\n",
      "Epoch: 376, Train Loss: 43.2441, Valid Loss: 65.3093\n",
      "Epoch: 377, Train Loss: 42.4008, Valid Loss: 62.5761\n",
      "Epoch: 378, Train Loss: 43.8093, Valid Loss: 62.0925\n",
      "Epoch: 379, Train Loss: 44.8836, Valid Loss: 63.3087\n",
      "Epoch: 380, Train Loss: 46.3045, Valid Loss: 83.8240\n",
      "Epoch: 381, Train Loss: 45.4239, Valid Loss: 60.8416\n",
      "Epoch: 382, Train Loss: 41.9221, Valid Loss: 63.1994\n",
      "Epoch: 383, Train Loss: 41.8911, Valid Loss: 61.6350\n",
      "Epoch: 384, Train Loss: 42.2163, Valid Loss: 60.8304\n",
      "Epoch: 385, Train Loss: 44.0113, Valid Loss: 64.4789\n",
      "Epoch: 386, Train Loss: 42.4978, Valid Loss: 64.7561\n",
      "Epoch: 387, Train Loss: 44.3298, Valid Loss: 62.0069\n",
      "Epoch: 388, Train Loss: 42.6671, Valid Loss: 61.1736\n",
      "Epoch: 389, Train Loss: 44.2727, Valid Loss: 62.2189\n",
      "Epoch: 390, Train Loss: 42.2630, Valid Loss: 59.8644\n",
      "Epoch: 391, Train Loss: 41.5555, Valid Loss: 62.1410\n",
      "Epoch: 392, Train Loss: 42.2204, Valid Loss: 63.1799\n",
      "Epoch: 393, Train Loss: 42.0417, Valid Loss: 59.8016\n",
      "Epoch: 394, Train Loss: 41.9630, Valid Loss: 61.4261\n",
      "Epoch: 395, Train Loss: 41.5059, Valid Loss: 63.5118\n",
      "Epoch: 396, Train Loss: 43.9453, Valid Loss: 100.9588\n",
      "Epoch: 397, Train Loss: 42.9275, Valid Loss: 64.0418\n",
      "Epoch: 398, Train Loss: 42.3682, Valid Loss: 62.3903\n",
      "Epoch: 399, Train Loss: 41.1261, Valid Loss: 62.5309\n",
      "Epoch: 400, Train Loss: 41.8317, Valid Loss: 62.9345\n",
      "Epoch: 401, Train Loss: 42.6191, Valid Loss: 61.7865\n",
      "Epoch: 402, Train Loss: 46.3065, Valid Loss: 59.4565\n",
      "Epoch: 403, Train Loss: 42.9283, Valid Loss: 64.5214\n",
      "Epoch: 404, Train Loss: 41.5231, Valid Loss: 59.8312\n",
      "Epoch: 405, Train Loss: 40.9371, Valid Loss: 61.9163\n",
      "Epoch: 406, Train Loss: 41.5485, Valid Loss: 62.6035\n",
      "Epoch: 407, Train Loss: 41.3258, Valid Loss: 62.8134\n",
      "Epoch: 408, Train Loss: 41.8458, Valid Loss: 61.3697\n",
      "Epoch: 409, Train Loss: 42.5328, Valid Loss: 65.1510\n",
      "Epoch: 410, Train Loss: 41.5257, Valid Loss: 61.4010\n",
      "Epoch: 411, Train Loss: 40.7662, Valid Loss: 61.0474\n",
      "Epoch: 412, Train Loss: 41.2652, Valid Loss: 57.7357\n",
      "Epoch: 413, Train Loss: 41.0922, Valid Loss: 61.2481\n",
      "Epoch: 414, Train Loss: 41.7954, Valid Loss: 59.3054\n",
      "Epoch: 415, Train Loss: 41.8034, Valid Loss: 62.8273\n",
      "Epoch: 416, Train Loss: 41.4360, Valid Loss: 57.9144\n",
      "Epoch: 417, Train Loss: 42.1428, Valid Loss: 60.9456\n",
      "Epoch: 418, Train Loss: 42.1999, Valid Loss: 60.0249\n",
      "Epoch: 419, Train Loss: 41.3090, Valid Loss: 59.2479\n",
      "Epoch: 420, Train Loss: 41.0620, Valid Loss: 60.7633\n",
      "Epoch: 421, Train Loss: 42.1500, Valid Loss: 59.9756\n",
      "Epoch: 422, Train Loss: 41.3702, Valid Loss: 62.8876\n",
      "Epoch: 423, Train Loss: 41.0190, Valid Loss: 60.1529\n",
      "Epoch: 424, Train Loss: 42.0767, Valid Loss: 60.1223\n",
      "Epoch: 425, Train Loss: 40.0929, Valid Loss: 63.2348\n",
      "Epoch: 426, Train Loss: 41.7873, Valid Loss: 58.3600\n",
      "Epoch: 427, Train Loss: 40.6534, Valid Loss: 60.7629\n",
      "Epoch: 428, Train Loss: 40.4626, Valid Loss: 60.8587\n",
      "Epoch: 429, Train Loss: 40.5383, Valid Loss: 61.4736\n",
      "Epoch: 430, Train Loss: 40.0515, Valid Loss: 58.6307\n",
      "Epoch: 431, Train Loss: 44.2167, Valid Loss: 61.8815\n",
      "Epoch: 432, Train Loss: 43.4200, Valid Loss: 62.6995\n",
      "Epoch: 433, Train Loss: 39.8004, Valid Loss: 63.3004\n",
      "Epoch: 434, Train Loss: 41.0543, Valid Loss: 63.0314\n",
      "Epoch: 435, Train Loss: 39.8625, Valid Loss: 57.0462\n",
      "Epoch: 436, Train Loss: 40.0831, Valid Loss: 67.0992\n",
      "Epoch: 437, Train Loss: 45.6079, Valid Loss: 74.2743\n",
      "Epoch: 438, Train Loss: 42.7794, Valid Loss: 58.5908\n",
      "Epoch: 439, Train Loss: 39.3161, Valid Loss: 59.1249\n",
      "Epoch: 440, Train Loss: 39.4734, Valid Loss: 58.4130\n",
      "Epoch: 441, Train Loss: 40.2066, Valid Loss: 61.5077\n",
      "Epoch: 442, Train Loss: 39.9526, Valid Loss: 58.8958\n",
      "Epoch: 443, Train Loss: 39.5683, Valid Loss: 61.0973\n",
      "Epoch: 444, Train Loss: 39.9408, Valid Loss: 60.4741\n",
      "Epoch: 445, Train Loss: 40.7021, Valid Loss: 60.7587\n",
      "Epoch: 446, Train Loss: 39.6814, Valid Loss: 57.8349\n",
      "Epoch: 447, Train Loss: 39.1261, Valid Loss: 67.0892\n",
      "Epoch: 448, Train Loss: 41.2820, Valid Loss: 58.9500\n",
      "Epoch: 449, Train Loss: 40.7000, Valid Loss: 55.7290\n",
      "Epoch: 450, Train Loss: 38.7414, Valid Loss: 59.8197\n",
      "Epoch: 451, Train Loss: 39.4172, Valid Loss: 58.4796\n",
      "Epoch: 452, Train Loss: 39.7457, Valid Loss: 59.2306\n",
      "Epoch: 453, Train Loss: 40.6707, Valid Loss: 60.6833\n",
      "Epoch: 454, Train Loss: 40.6476, Valid Loss: 58.9947\n",
      "Epoch: 455, Train Loss: 39.5744, Valid Loss: 59.0886\n",
      "Epoch: 456, Train Loss: 39.7449, Valid Loss: 57.1321\n",
      "Epoch: 457, Train Loss: 39.4179, Valid Loss: 70.5611\n",
      "Epoch: 458, Train Loss: 39.9290, Valid Loss: 60.2064\n",
      "Epoch: 459, Train Loss: 39.5978, Valid Loss: 60.8991\n",
      "Epoch: 460, Train Loss: 39.0024, Valid Loss: 58.6626\n",
      "Epoch: 461, Train Loss: 39.2980, Valid Loss: 62.2581\n",
      "Epoch: 462, Train Loss: 39.7531, Valid Loss: 60.0152\n",
      "Epoch: 463, Train Loss: 39.9569, Valid Loss: 65.7911\n",
      "Epoch: 464, Train Loss: 39.1309, Valid Loss: 58.1320\n",
      "Epoch: 465, Train Loss: 39.3234, Valid Loss: 59.7966\n",
      "Epoch: 466, Train Loss: 39.1421, Valid Loss: 57.6228\n",
      "Epoch: 467, Train Loss: 39.5824, Valid Loss: 61.6693\n",
      "Epoch: 468, Train Loss: 39.7275, Valid Loss: 59.6066\n",
      "Epoch: 469, Train Loss: 39.2532, Valid Loss: 59.2096\n",
      "Epoch: 470, Train Loss: 41.5270, Valid Loss: 57.7433\n",
      "Epoch: 471, Train Loss: 38.1494, Valid Loss: 56.9965\n",
      "Epoch: 472, Train Loss: 38.1642, Valid Loss: 56.9636\n",
      "Epoch: 473, Train Loss: 38.8706, Valid Loss: 56.7372\n",
      "Epoch: 474, Train Loss: 38.2024, Valid Loss: 59.7352\n",
      "Epoch: 475, Train Loss: 38.4247, Valid Loss: 60.1542\n",
      "Epoch: 476, Train Loss: 37.7404, Valid Loss: 58.5490\n",
      "Epoch: 477, Train Loss: 39.1544, Valid Loss: 62.8136\n",
      "Epoch: 478, Train Loss: 38.8414, Valid Loss: 60.0345\n",
      "Epoch: 479, Train Loss: 41.7051, Valid Loss: 57.4303\n",
      "Epoch: 480, Train Loss: 39.0530, Valid Loss: 66.9282\n",
      "Epoch: 481, Train Loss: 39.2066, Valid Loss: 56.8637\n",
      "Epoch: 482, Train Loss: 38.6983, Valid Loss: 58.4292\n",
      "Epoch: 483, Train Loss: 37.4986, Valid Loss: 57.7116\n",
      "Epoch: 484, Train Loss: 38.7513, Valid Loss: 58.0792\n",
      "Epoch: 485, Train Loss: 38.7864, Valid Loss: 56.7079\n",
      "Epoch: 486, Train Loss: 37.7321, Valid Loss: 58.6962\n",
      "Epoch: 487, Train Loss: 38.2617, Valid Loss: 59.0888\n",
      "Epoch: 488, Train Loss: 37.2296, Valid Loss: 56.4580\n",
      "Epoch: 489, Train Loss: 37.5748, Valid Loss: 56.1759\n",
      "Epoch: 490, Train Loss: 38.5150, Valid Loss: 57.8038\n",
      "Epoch: 491, Train Loss: 38.6292, Valid Loss: 56.2220\n",
      "Epoch: 492, Train Loss: 38.7352, Valid Loss: 55.7967\n",
      "Epoch: 493, Train Loss: 37.4313, Valid Loss: 55.7415\n",
      "Epoch: 494, Train Loss: 38.0616, Valid Loss: 56.3071\n",
      "Epoch: 495, Train Loss: 37.6406, Valid Loss: 59.4415\n",
      "Epoch: 496, Train Loss: 38.3471, Valid Loss: 57.1949\n",
      "Epoch: 497, Train Loss: 39.3215, Valid Loss: 61.9415\n",
      "Epoch: 498, Train Loss: 37.7387, Valid Loss: 57.2360\n",
      "Epoch: 499, Train Loss: 37.9332, Valid Loss: 58.7137\n",
      "Learning rate has been changed to: 0.0001\n",
      "Epoch: 500, Train Loss: 38.8491, Valid Loss: 61.3484\n",
      "Epoch: 501, Train Loss: 33.2061, Valid Loss: 53.5580\n",
      "Epoch: 502, Train Loss: 31.9565, Valid Loss: 53.4001\n",
      "Epoch: 503, Train Loss: 31.5381, Valid Loss: 53.9788\n",
      "Epoch: 504, Train Loss: 31.4446, Valid Loss: 53.6205\n",
      "Epoch: 505, Train Loss: 31.2685, Valid Loss: 53.7918\n",
      "Epoch: 506, Train Loss: 31.2481, Valid Loss: 53.0242\n",
      "Epoch: 507, Train Loss: 31.1768, Valid Loss: 53.0157\n",
      "Epoch: 508, Train Loss: 31.1370, Valid Loss: 53.4480\n",
      "Epoch: 509, Train Loss: 31.0432, Valid Loss: 53.4971\n",
      "Epoch: 510, Train Loss: 31.1117, Valid Loss: 53.6911\n",
      "Epoch: 511, Train Loss: 31.0500, Valid Loss: 53.4913\n",
      "Epoch: 512, Train Loss: 30.9226, Valid Loss: 54.1761\n",
      "Epoch: 513, Train Loss: 30.9288, Valid Loss: 53.7714\n",
      "Epoch: 514, Train Loss: 30.8751, Valid Loss: 54.0741\n",
      "Epoch: 515, Train Loss: 30.8593, Valid Loss: 53.1729\n",
      "Epoch: 516, Train Loss: 30.7894, Valid Loss: 53.7801\n",
      "Epoch: 517, Train Loss: 30.7444, Valid Loss: 54.5127\n",
      "Epoch: 518, Train Loss: 30.8230, Valid Loss: 53.1325\n",
      "Epoch: 519, Train Loss: 30.7655, Valid Loss: 53.4051\n",
      "Epoch: 520, Train Loss: 30.7303, Valid Loss: 54.8677\n",
      "Epoch: 521, Train Loss: 30.6598, Valid Loss: 53.8671\n",
      "Epoch: 522, Train Loss: 30.6502, Valid Loss: 53.5302\n",
      "Epoch: 523, Train Loss: 30.6510, Valid Loss: 53.6577\n",
      "Epoch: 524, Train Loss: 30.5873, Valid Loss: 53.4135\n",
      "Epoch: 525, Train Loss: 30.5918, Valid Loss: 54.2669\n",
      "Epoch: 526, Train Loss: 30.6587, Valid Loss: 54.2811\n",
      "Epoch: 527, Train Loss: 30.5660, Valid Loss: 52.8975\n",
      "Epoch: 528, Train Loss: 30.5675, Valid Loss: 53.4160\n",
      "Epoch: 529, Train Loss: 30.5653, Valid Loss: 53.4069\n",
      "Epoch: 530, Train Loss: 30.4352, Valid Loss: 53.2596\n",
      "Epoch: 531, Train Loss: 30.4428, Valid Loss: 53.6109\n",
      "Epoch: 532, Train Loss: 30.5125, Valid Loss: 53.9160\n",
      "Epoch: 533, Train Loss: 30.4741, Valid Loss: 54.0951\n",
      "Epoch: 534, Train Loss: 30.3836, Valid Loss: 54.2977\n",
      "Epoch: 535, Train Loss: 30.3868, Valid Loss: 53.9678\n",
      "Epoch: 536, Train Loss: 30.4716, Valid Loss: 53.8980\n",
      "Epoch: 537, Train Loss: 30.3685, Valid Loss: 54.1895\n",
      "Epoch: 538, Train Loss: 30.3421, Valid Loss: 53.5306\n",
      "Epoch: 539, Train Loss: 30.4015, Valid Loss: 53.3436\n",
      "Epoch: 540, Train Loss: 30.3583, Valid Loss: 54.0054\n",
      "Epoch: 541, Train Loss: 30.2664, Valid Loss: 53.6714\n",
      "Epoch: 542, Train Loss: 30.2495, Valid Loss: 53.9468\n",
      "Epoch: 543, Train Loss: 30.2710, Valid Loss: 53.3653\n",
      "Epoch: 544, Train Loss: 30.2579, Valid Loss: 53.6309\n",
      "Epoch: 545, Train Loss: 30.2705, Valid Loss: 54.4282\n",
      "Epoch: 546, Train Loss: 30.2617, Valid Loss: 53.0846\n",
      "Epoch: 547, Train Loss: 30.1191, Valid Loss: 53.5927\n",
      "Epoch: 548, Train Loss: 30.1565, Valid Loss: 53.7632\n",
      "Epoch: 549, Train Loss: 30.1565, Valid Loss: 54.0843\n",
      "Epoch: 550, Train Loss: 30.1676, Valid Loss: 53.7158\n",
      "Epoch: 551, Train Loss: 30.1410, Valid Loss: 54.3557\n",
      "Epoch: 552, Train Loss: 30.1461, Valid Loss: 53.2574\n",
      "Epoch: 553, Train Loss: 30.0992, Valid Loss: 55.0597\n",
      "Epoch: 554, Train Loss: 30.1854, Valid Loss: 54.0727\n",
      "Epoch: 555, Train Loss: 30.0987, Valid Loss: 53.3643\n",
      "Epoch: 556, Train Loss: 30.0582, Valid Loss: 53.2238\n",
      "Epoch: 557, Train Loss: 30.1288, Valid Loss: 53.2570\n",
      "Epoch: 558, Train Loss: 30.1139, Valid Loss: 54.1624\n",
      "Epoch: 559, Train Loss: 30.0469, Valid Loss: 54.1661\n",
      "Epoch: 560, Train Loss: 30.0255, Valid Loss: 53.6727\n",
      "Epoch: 561, Train Loss: 30.0867, Valid Loss: 54.5642\n",
      "Epoch: 562, Train Loss: 30.0256, Valid Loss: 53.2251\n",
      "Epoch: 563, Train Loss: 29.9671, Valid Loss: 53.1454\n",
      "Epoch: 564, Train Loss: 30.0058, Valid Loss: 53.5405\n",
      "Epoch: 565, Train Loss: 29.9736, Valid Loss: 53.5536\n",
      "Epoch: 566, Train Loss: 29.9673, Valid Loss: 53.4888\n",
      "Epoch: 567, Train Loss: 30.0175, Valid Loss: 54.7261\n",
      "Epoch: 568, Train Loss: 29.9147, Valid Loss: 54.1975\n",
      "Epoch: 569, Train Loss: 29.9085, Valid Loss: 54.3902\n",
      "Epoch: 570, Train Loss: 29.8710, Valid Loss: 52.4761\n",
      "Epoch: 571, Train Loss: 29.8575, Valid Loss: 53.7198\n",
      "Epoch: 572, Train Loss: 29.9175, Valid Loss: 53.5776\n",
      "Epoch: 573, Train Loss: 29.9624, Valid Loss: 53.9637\n",
      "Epoch: 574, Train Loss: 29.8019, Valid Loss: 53.1130\n",
      "Epoch: 575, Train Loss: 29.8239, Valid Loss: 53.4563\n",
      "Epoch: 576, Train Loss: 29.7954, Valid Loss: 53.5799\n",
      "Epoch: 577, Train Loss: 29.8004, Valid Loss: 53.6541\n",
      "Epoch: 578, Train Loss: 29.7557, Valid Loss: 53.0476\n",
      "Epoch: 579, Train Loss: 29.8241, Valid Loss: 53.2833\n",
      "Epoch: 580, Train Loss: 29.8038, Valid Loss: 53.0733\n",
      "Epoch: 581, Train Loss: 29.7452, Valid Loss: 53.2016\n",
      "Epoch: 582, Train Loss: 29.7712, Valid Loss: 53.7569\n",
      "Epoch: 583, Train Loss: 29.7237, Valid Loss: 53.3521\n",
      "Epoch: 584, Train Loss: 29.7168, Valid Loss: 52.8060\n",
      "Epoch: 585, Train Loss: 29.6990, Valid Loss: 53.3417\n",
      "Epoch: 586, Train Loss: 29.6752, Valid Loss: 52.8204\n",
      "Epoch: 587, Train Loss: 29.6531, Valid Loss: 54.1777\n",
      "Epoch: 588, Train Loss: 29.8119, Valid Loss: 52.9282\n",
      "Epoch: 589, Train Loss: 29.6196, Valid Loss: 53.3622\n",
      "Epoch: 590, Train Loss: 29.6888, Valid Loss: 54.0337\n",
      "Epoch: 591, Train Loss: 29.5837, Valid Loss: 54.2701\n",
      "Epoch: 592, Train Loss: 29.6728, Valid Loss: 53.8005\n",
      "Epoch: 593, Train Loss: 29.6370, Valid Loss: 53.4786\n",
      "Epoch: 594, Train Loss: 29.6490, Valid Loss: 53.9761\n",
      "Epoch: 595, Train Loss: 29.6693, Valid Loss: 54.2237\n",
      "Epoch: 596, Train Loss: 29.5781, Valid Loss: 53.6914\n",
      "Epoch: 597, Train Loss: 29.6685, Valid Loss: 54.9152\n",
      "Epoch: 598, Train Loss: 29.6031, Valid Loss: 54.0577\n",
      "Epoch: 599, Train Loss: 29.5860, Valid Loss: 52.9266\n",
      "Epoch: 600, Train Loss: 29.5139, Valid Loss: 54.2629\n",
      "Epoch: 601, Train Loss: 29.5592, Valid Loss: 53.7416\n",
      "Epoch: 602, Train Loss: 29.5515, Valid Loss: 53.7280\n",
      "Epoch: 603, Train Loss: 29.5420, Valid Loss: 54.0239\n",
      "Epoch: 604, Train Loss: 29.5709, Valid Loss: 53.0020\n",
      "Epoch: 605, Train Loss: 29.4979, Valid Loss: 53.5354\n",
      "Epoch: 606, Train Loss: 29.5194, Valid Loss: 53.1330\n",
      "Epoch: 607, Train Loss: 29.4509, Valid Loss: 53.7664\n",
      "Epoch: 608, Train Loss: 29.4503, Valid Loss: 54.6278\n",
      "Epoch: 609, Train Loss: 29.4568, Valid Loss: 53.6714\n",
      "Epoch: 610, Train Loss: 29.3953, Valid Loss: 53.5835\n",
      "Epoch: 611, Train Loss: 29.4745, Valid Loss: 53.7994\n",
      "Epoch: 612, Train Loss: 29.4347, Valid Loss: 53.8883\n",
      "Epoch: 613, Train Loss: 29.4291, Valid Loss: 53.9478\n",
      "Epoch: 614, Train Loss: 29.4403, Valid Loss: 53.7373\n",
      "Epoch: 615, Train Loss: 29.4687, Valid Loss: 53.6118\n",
      "Epoch: 616, Train Loss: 29.4100, Valid Loss: 52.9051\n",
      "Epoch: 617, Train Loss: 29.4418, Valid Loss: 54.0017\n",
      "Epoch: 618, Train Loss: 29.3724, Valid Loss: 53.1308\n",
      "Epoch: 619, Train Loss: 29.3355, Valid Loss: 52.6700\n",
      "Epoch: 620, Train Loss: 29.3226, Valid Loss: 53.6517\n",
      "Learning rate has been changed to: 1e-05\n",
      "Epoch: 621, Train Loss: 29.3359, Valid Loss: 53.3712\n",
      "Epoch: 622, Train Loss: 28.7943, Valid Loss: 53.2979\n",
      "Epoch: 623, Train Loss: 28.6846, Valid Loss: 53.3876\n",
      "Epoch: 624, Train Loss: 28.6605, Valid Loss: 53.3759\n",
      "Epoch: 625, Train Loss: 28.6549, Valid Loss: 53.4578\n",
      "Epoch: 626, Train Loss: 28.6501, Valid Loss: 53.3033\n",
      "Epoch: 627, Train Loss: 28.6482, Valid Loss: 53.4483\n",
      "Epoch: 628, Train Loss: 28.6369, Valid Loss: 53.4222\n",
      "Epoch: 629, Train Loss: 28.6392, Valid Loss: 53.2871\n",
      "Epoch: 630, Train Loss: 28.6370, Valid Loss: 53.4581\n",
      "Epoch: 631, Train Loss: 28.6317, Valid Loss: 53.2957\n",
      "Epoch: 632, Train Loss: 28.6449, Valid Loss: 53.3519\n",
      "Epoch: 633, Train Loss: 28.6351, Valid Loss: 53.4136\n",
      "Epoch: 634, Train Loss: 28.6371, Valid Loss: 53.4168\n",
      "Epoch: 635, Train Loss: 28.6302, Valid Loss: 53.4662\n",
      "Epoch: 636, Train Loss: 28.6317, Valid Loss: 53.2954\n",
      "Epoch: 637, Train Loss: 28.6279, Valid Loss: 53.3732\n",
      "Epoch: 638, Train Loss: 28.6184, Valid Loss: 53.5800\n",
      "Epoch: 639, Train Loss: 28.6273, Valid Loss: 53.4780\n",
      "Epoch: 640, Train Loss: 28.6065, Valid Loss: 53.4302\n",
      "Epoch: 641, Train Loss: 28.6162, Valid Loss: 53.4922\n",
      "Epoch: 642, Train Loss: 28.6260, Valid Loss: 53.4853\n",
      "Epoch: 643, Train Loss: 28.6116, Valid Loss: 53.4734\n",
      "Epoch: 644, Train Loss: 28.6223, Valid Loss: 53.4458\n",
      "Epoch: 645, Train Loss: 28.6205, Valid Loss: 53.4842\n",
      "Epoch: 646, Train Loss: 28.6141, Valid Loss: 53.4528\n",
      "Epoch: 647, Train Loss: 28.6047, Valid Loss: 53.3935\n",
      "Epoch: 648, Train Loss: 28.6083, Valid Loss: 53.4850\n",
      "Epoch: 649, Train Loss: 28.6095, Valid Loss: 53.3152\n",
      "Epoch: 650, Train Loss: 28.6120, Valid Loss: 53.3933\n",
      "Epoch: 651, Train Loss: 28.6015, Valid Loss: 53.3531\n",
      "Epoch: 652, Train Loss: 28.5975, Valid Loss: 53.4401\n",
      "Epoch: 653, Train Loss: 28.5951, Valid Loss: 53.6018\n",
      "Epoch: 654, Train Loss: 28.6113, Valid Loss: 53.3869\n",
      "Epoch: 655, Train Loss: 28.5983, Valid Loss: 53.3578\n",
      "Epoch: 656, Train Loss: 28.6061, Valid Loss: 53.5137\n",
      "Epoch: 657, Train Loss: 28.5904, Valid Loss: 53.5061\n",
      "Epoch: 658, Train Loss: 28.5991, Valid Loss: 53.3455\n",
      "Epoch: 659, Train Loss: 28.6009, Valid Loss: 53.4273\n",
      "Epoch: 660, Train Loss: 28.5901, Valid Loss: 53.3462\n",
      "Epoch: 661, Train Loss: 28.5907, Valid Loss: 53.3422\n",
      "Epoch: 662, Train Loss: 28.5854, Valid Loss: 53.5248\n",
      "Epoch: 663, Train Loss: 28.5980, Valid Loss: 53.3233\n",
      "Epoch: 664, Train Loss: 28.5918, Valid Loss: 53.4641\n",
      "Epoch: 665, Train Loss: 28.5878, Valid Loss: 53.4417\n",
      "Epoch: 666, Train Loss: 28.5922, Valid Loss: 53.3879\n",
      "Epoch: 667, Train Loss: 28.5915, Valid Loss: 53.3436\n",
      "Epoch: 668, Train Loss: 28.5826, Valid Loss: 53.3438\n",
      "Epoch: 669, Train Loss: 28.5828, Valid Loss: 53.2840\n",
      "Epoch: 670, Train Loss: 28.5845, Valid Loss: 53.3972\n",
      "Epoch: 671, Train Loss: 28.5852, Valid Loss: 53.4797\n",
      "Learning rate has been changed to: 1.0000000000000002e-06\n",
      "Epoch: 672, Train Loss: 28.5807, Valid Loss: 53.3793\n",
      "Epoch: 673, Train Loss: 28.5085, Valid Loss: 53.3739\n",
      "Epoch: 674, Train Loss: 28.5002, Valid Loss: 53.3744\n",
      "Epoch: 675, Train Loss: 28.4983, Valid Loss: 53.3745\n",
      "Epoch: 676, Train Loss: 28.4968, Valid Loss: 53.3839\n",
      "Epoch: 677, Train Loss: 28.5002, Valid Loss: 53.3811\n",
      "Epoch: 678, Train Loss: 28.4952, Valid Loss: 53.3934\n",
      "Epoch: 679, Train Loss: 28.4973, Valid Loss: 53.3955\n",
      "Epoch: 680, Train Loss: 28.4955, Valid Loss: 53.3986\n",
      "Epoch: 681, Train Loss: 28.4971, Valid Loss: 53.3901\n",
      "Epoch: 682, Train Loss: 28.4969, Valid Loss: 53.4013\n",
      "Epoch: 683, Train Loss: 28.4940, Valid Loss: 53.4089\n",
      "Epoch: 684, Train Loss: 28.4961, Valid Loss: 53.4047\n",
      "Epoch: 685, Train Loss: 28.4955, Valid Loss: 53.3926\n",
      "Epoch: 686, Train Loss: 28.4962, Valid Loss: 53.4024\n",
      "Epoch: 687, Train Loss: 28.4959, Valid Loss: 53.4113\n",
      "Epoch: 688, Train Loss: 28.4938, Valid Loss: 53.4033\n",
      "Epoch: 689, Train Loss: 28.4960, Valid Loss: 53.3988\n",
      "Epoch: 690, Train Loss: 28.4958, Valid Loss: 53.4020\n",
      "Epoch: 691, Train Loss: 28.4961, Valid Loss: 53.4088\n",
      "Epoch: 692, Train Loss: 28.4938, Valid Loss: 53.4085\n",
      "Epoch: 693, Train Loss: 28.4942, Valid Loss: 53.4116\n",
      "Epoch: 694, Train Loss: 28.4926, Valid Loss: 53.4183\n",
      "Epoch: 695, Train Loss: 28.4930, Valid Loss: 53.4152\n",
      "Epoch: 696, Train Loss: 28.4954, Valid Loss: 53.4190\n",
      "Epoch: 697, Train Loss: 28.4934, Valid Loss: 53.4168\n",
      "Epoch: 698, Train Loss: 28.4936, Valid Loss: 53.4153\n",
      "Epoch: 699, Train Loss: 28.4920, Valid Loss: 53.4092\n",
      "Epoch: 700, Train Loss: 28.4943, Valid Loss: 53.4133\n",
      "Epoch: 701, Train Loss: 28.4912, Valid Loss: 53.4174\n",
      "Epoch: 702, Train Loss: 28.4936, Valid Loss: 53.4072\n",
      "Epoch: 703, Train Loss: 28.4955, Valid Loss: 53.4157\n",
      "Epoch: 704, Train Loss: 28.4911, Valid Loss: 53.4164\n",
      "Epoch: 705, Train Loss: 28.4914, Valid Loss: 53.4121\n",
      "Epoch: 706, Train Loss: 28.4932, Valid Loss: 53.4318\n",
      "Epoch: 707, Train Loss: 28.4933, Valid Loss: 53.4227\n",
      "Epoch: 708, Train Loss: 28.4918, Valid Loss: 53.4147\n",
      "Epoch: 709, Train Loss: 28.4959, Valid Loss: 53.4071\n",
      "Epoch: 710, Train Loss: 28.4939, Valid Loss: 53.4117\n",
      "Epoch: 711, Train Loss: 28.4923, Valid Loss: 53.4177\n",
      "Epoch: 712, Train Loss: 28.4928, Valid Loss: 53.4183\n",
      "Epoch: 713, Train Loss: 28.4920, Valid Loss: 53.4168\n",
      "Epoch: 714, Train Loss: 28.4935, Valid Loss: 53.4288\n",
      "Epoch: 715, Train Loss: 28.4922, Valid Loss: 53.4195\n",
      "Epoch: 716, Train Loss: 28.4914, Valid Loss: 53.4144\n",
      "Epoch: 717, Train Loss: 28.4921, Valid Loss: 53.4268\n",
      "Epoch: 718, Train Loss: 28.4920, Valid Loss: 53.4278\n",
      "Epoch: 719, Train Loss: 28.4900, Valid Loss: 53.4303\n",
      "Epoch: 720, Train Loss: 28.4933, Valid Loss: 53.4249\n",
      "Epoch: 721, Train Loss: 28.4930, Valid Loss: 53.4139\n",
      "Epoch: 722, Train Loss: 28.4919, Valid Loss: 53.4156\n",
      "Learning rate has been changed to: 1.0000000000000002e-07\n",
      "Epoch: 723, Train Loss: 28.4896, Valid Loss: 53.4259\n"
     ]
    }
   ],
   "source": [
    "#直接训练模型，根据验证集MSE保存最优模型，超过50伦验证集MSE没有下降则修改学习率\n",
    "min_valid_loss = float('inf')\n",
    "no_improve = 0\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for features, targets in train_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model = model.cuda()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss = train_loss+loss.item()\n",
    "    valid_loss = valid(model, valid_data, criterion)\n",
    "    if valid_loss < min_valid_loss:\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(model, model_dir + str(min_valid_loss) + '.pth')\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "    if no_improve > 50:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "            print('Learning rate has been changed to: {}'.format(param_group['lr']))\n",
    "        no_improve = 0\n",
    "    print('Epoch: {}, Train Loss: {:.4f}, Valid Loss: {:.4f}'.format(epoch, train_loss / len(train_data), valid_loss))\n",
    "    if optimizer.param_groups[0]['lr'] < 1e-6:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4447\n"
     ]
    }
   ],
   "source": [
    "# model=torch.load(\"model/CNN_Transformer240.1402118945944.pth\")\n",
    "model=torch.load(\"model/CNN_BiGRU_Attention52.89754729934886.pth\")\n",
    "#计算测试集MSE\n",
    "test_data_dir=\"data/test_data.xlsx\"\n",
    "test_data_loader = Test_Loader(test_data_dir, train_data_loader.scaler)\n",
    "test_data = DataLoader(test_data_loader, batch_size=1, shuffle=False)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YS_RMSE: 8.8894,YS_MAPE: 0.0235,YS_R2: 0.9935\n",
      "TS_RMSE: 9.2999,TS_MAPE: 0.0145,TS_R2: 0.9963\n",
      "EL_RMSE: 1.8741,EL_MAPE: 0.0395,EL_R2: 0.9611\n"
     ]
    }
   ],
   "source": [
    "#计算测试集上的MSE,RMSE,MAE,R2\n",
    "def test(test_model, test_data):\n",
    "    test_model.eval()\n",
    "\n",
    "    YS_true = []\n",
    "    YS_pred = []\n",
    "\n",
    "    TS_true = []\n",
    "    TS_pred = []\n",
    "\n",
    "    EL_true = []\n",
    "    EL_pred = []\n",
    "\n",
    "    for features, targets in test_data:\n",
    "        features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        test_model = test_model.cuda()\n",
    "        outputs = test_model(features)\n",
    "        YS_true.append(targets.cpu().detach().numpy()[0][0][0])\n",
    "        YS_pred.append(outputs.cpu().detach().numpy()[0][0][0])\n",
    "        TS_true.append(targets.cpu().detach().numpy()[0][0][1])\n",
    "        TS_pred.append(outputs.cpu().detach().numpy()[0][0][1])\n",
    "        EL_true.append(targets.cpu().detach().numpy()[0][0][2])\n",
    "        EL_pred.append(outputs.cpu().detach().numpy()[0][0][2])\n",
    "    #计算YS的RMSE,MAPE,R2\n",
    "    YS_true = np.array(YS_true)\n",
    "    YS_pred = np.array(YS_pred)\n",
    "    YS_MSE = mean_squared_error(YS_true, YS_pred)\n",
    "    YS_RMSE = np.sqrt(YS_MSE)\n",
    "    YS_MAPE = np.mean(np.abs((YS_pred - YS_true) / YS_true))\n",
    "    YS_R2 = 1 - YS_MSE / np.var(YS_true)\n",
    "    print(f'YS_RMSE: {YS_RMSE:.4f},YS_MAPE: {YS_MAPE:.4f},YS_R2: {YS_R2:.4f}')\n",
    "    #计算TS的RMSE,MAPE,R2\n",
    "    TS_true = np.array(TS_true)\n",
    "    TS_pred = np.array(TS_pred)\n",
    "    TS_MSE = mean_squared_error(TS_true, TS_pred)\n",
    "    TS_RMSE = np.sqrt(TS_MSE)\n",
    "    TS_MAPE = np.mean(np.abs((TS_pred - TS_true) / TS_true))\n",
    "    TS_R2 = 1 - TS_MSE / np.var(TS_true)\n",
    "    print(f'TS_RMSE: {TS_RMSE:.4f},TS_MAPE: {TS_MAPE:.4f},TS_R2: {TS_R2:.4f}')\n",
    "    #计算EL的RMSE,MAPE,R2\n",
    "    EL_true = np.array(EL_true)\n",
    "    EL_pred = np.array(EL_pred)\n",
    "    EL_MSE = mean_squared_error(EL_true, EL_pred)\n",
    "    EL_RMSE = np.sqrt(EL_MSE)\n",
    "    EL_MAPE = np.mean(np.abs((EL_pred - EL_true) / EL_true))\n",
    "    EL_R2 = 1 - EL_MSE / np.var(EL_true)\n",
    "    print(f'EL_RMSE: {EL_RMSE:.4f},EL_MAPE: {EL_MAPE:.4f},EL_R2: {EL_R2:.4f}')\n",
    "    \n",
    "\n",
    "    return YS_pred, TS_pred, EL_pred, YS_true, TS_true, EL_true\n",
    "\n",
    "YS_pred, TS_pred, EL_pred, YS_true, TS_true, EL_true = test(model, test_data)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch121",
   "language": "python",
   "name": "pytorch121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
